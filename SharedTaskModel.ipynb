{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a0cac7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a7371e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0afd5",
   "metadata": {},
   "source": [
    "Prepping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f257209f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "['A26004', 'We should end affirmative action', 'against', 'affirmative action helps with employment equity.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data/arguments-training.tsv\", 'r', encoding='utf8')\n",
    "x_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_train[0])\n",
    "\n",
    "train_df = pd.read_csv('data/labels-training.tsv',encoding ='utf8')\n",
    "\n",
    "file =e.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "#prinle = open(\"data/arguments-validation.tsv\", 'r', encoding='utf8')\n",
    "x_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_valid[0])\n",
    "file = open(\"data/labels-validation.tsv\", 'r', encoding='utf8')\n",
    "y_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(y_valid[0])\n",
    "file = open(\"data/arguments-test.tsv\", 'r', encoding='utf8')\n",
    "x_test = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f3386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>Entrapment should be legalized</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>if entrapment can serve to more easily capture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01003</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>against</td>\n",
       "      <td>marriage is the ultimate commitment to someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01004</td>\n",
       "      <td>We should ban naturopathy</td>\n",
       "      <td>against</td>\n",
       "      <td>it provides a useful income for some people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID                      Conclusion       Stance  \\\n",
       "0      A01001  Entrapment should be legalized  in favor of   \n",
       "1      A01002     We should ban human cloning  in favor of   \n",
       "2      A01003      We should abandon marriage      against   \n",
       "3      A01004       We should ban naturopathy      against   \n",
       "4      A01005         We should ban fast food  in favor of   \n",
       "\n",
       "                                             Premise  \n",
       "0  if entrapment can serve to more easily capture...  \n",
       "1  we should ban human cloning as it will only ca...  \n",
       "2  marriage is the ultimate commitment to someone...  \n",
       "3        it provides a useful income for some people  \n",
       "4  fast food should be banned because it is reall...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = pd.read_csv('data/arguments-training.tsv', sep='\\t',encoding ='utf8')\n",
    "#x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ae5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_train size:  5220  - x_train size:  5220\n",
      "___________________\n",
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_valid size:  1896  - y_valid size:  1896\n",
      "_______________\n",
      "['<SOS>', We, should, end, affirmative, action, '<CON>', affirmative, action, helps, with, employment, equity, ., '<EOS>']\n",
      "xTest size:  1576\n"
     ]
    }
   ],
   "source": [
    "#Tokenize, conjoin strings, and add special tokens, remove item ids from labels\n",
    "import spacy\n",
    "\n",
    "def tokenize(text, labels=None):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    args = []\n",
    "    labs = []\n",
    "    if(labels != None):\n",
    "        for arg, lab in zip(text, labels):\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "            labs.append(lab[1:20])\n",
    "    else:\n",
    "        for arg in text:\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "\n",
    "    return args, labs\n",
    "    \n",
    "def tokenize_allData(x_train,y_train,x_valid,y_valid,x_test):\n",
    "    x_train, y_train = tokenize(x_train, y_train)\n",
    "    x_valid, y_valid = tokenize(x_valid,y_valid)\n",
    "    x_test, _ = tokenize(x_test)\n",
    "    print(x_train[0], y_train[0])\n",
    "    print(\"x_train size: \",len(x_train),\" - x_train size: \",len(y_train))\n",
    "    print(\"___________________\")\n",
    "    print(x_valid[0], y_valid[0])\n",
    "    print(\"x_valid size: \",len(x_valid),\" - y_valid size: \",len(y_valid))\n",
    "    print(\"_______________\")\n",
    "    print(x_test[0])\n",
    "    print(\"xTest size: \", len(x_test))\n",
    "    return x_train,y_train,x_valid,y_valid,x_test\n",
    "x_train,y_train,x_valid,y_valid,x_test = tokenize_allData(x_train,y_train,x_valid,y_valid,x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02699a90",
   "metadata": {},
   "source": [
    "## Label Selection (Which labels to use in models )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb00999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLabels_NotWanted(labels, labels_wanted):\n",
    "    newLabels=[]\n",
    "    for row in labels:\n",
    "        newRow = []\n",
    "        for i in labels_wanted:\n",
    "            newRow.append(row[i])\n",
    "        assert len(newRow) == len(labels_wanted)\n",
    "        newLabels.append(newRow)\n",
    "    return newLabels\n",
    "def removeAllEmptyLabelRows(text,labels):\n",
    "    newText=[]\n",
    "    newLabels=[]\n",
    "    for i in range(len(text)):\n",
    "        intList = [eval(j) for j in labels[i] ]\n",
    "        if(np.sum(intList)!=0):\n",
    "            newText.append(text[i])\n",
    "            newLabels.append(labels[i])\n",
    "      #else:\n",
    "            #print(labels[i])\n",
    "    return newText,newLabels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2087c5dd",
   "metadata": {},
   "source": [
    "### Creating different labels for training on     - labels_starters recommended by Eval:   Self-direction: action, Achievement, Security: personal, Security: societal, Benevolence: caring, Universalism: concern.\n",
    "    - labels_universalism all labels from the universalism category\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b6877e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original items: 5220\n",
      "Number of items with one of the desired labels: 3410\n"
     ]
    }
   ],
   "source": [
    "reference_labels = {\n",
    "    \"Self-direction: thought\": 0,\n",
    "    \"Self-direction: action\":1,\t\n",
    "    \"Stimulation\": 2,\n",
    "    \"Hedonism\": 3,\n",
    "    \"Achievement\": 4,\n",
    "    \"Power: dominance\": 5,\n",
    "    \"Power: resources\": 6,\n",
    "    \"Face\": 7,\t\n",
    "    \"Security: personal\": 8,\n",
    "    \"Security: societal\": 9,\n",
    "    \"Tradition\": 10,\n",
    "    \"Conformity: rules\": 11,\n",
    "    \"Conformity: interpersonal\": 12,\n",
    "    \"Humility\": 13,\t\n",
    "    \"Benevolence: caring\": 14,\n",
    "    \"Benevolence: dependability\": 15,\t\n",
    "    \"Universalism: concern\": 16,\t\n",
    "    \"Universalism: nature\": 17,\t\n",
    "    \"Universalism: tolerance\": 18,\n",
    "    \"Universalism: objectivity\": 19\n",
    "}\n",
    "\n",
    "recommended_categories = [1, 4, 8, 9, 14, 16]\n",
    "print(\"Number of original items:\", len(y_train))\n",
    "labels_starters = removeLabels_NotWanted(y_train, recommended_categories)\n",
    "x_train_trimmed, labels_starters = removeAllEmptyLabelRows(x_train, labels_starters)\n",
    "print(\"Number of items with one of the desired labels:\", len(labels_starters))\n",
    "\n",
    "starters_dict = {\n",
    "    \"Self-direction: action\":0,\n",
    "    \"Achievement\": 1,\n",
    "    \"Security: personal\": 2,\n",
    "    \"Security: societal\": 3,\n",
    "    \"Benevolence: caring\": 4,\n",
    "    \"Universalism: concern\": 16\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073bb90d",
   "metadata": {},
   "source": [
    "## Create P3 label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9bc45830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makePT3Labels(allClassLabels):\n",
    "    PT3LabelsDict = {}\n",
    "    PT3Labels = []\n",
    "    label=0\n",
    "    for row in allClassLabels:\n",
    "        row_lab = [key for key, value in PT3LabelsDict.items() if value == row]\n",
    "        if row_lab:\n",
    "            PT3Labels.append(row_lab[0])\n",
    "        else:\n",
    "            PT3LabelsDict[label] = row\n",
    "            PT3Labels.append(label)\n",
    "            label += 1\n",
    "   \n",
    "    return PT3Labels, PT3LabelsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "64791c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT3Labels, PT3Dict = makePT3Labels(labels_starters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eac4a03",
   "metadata": {},
   "source": [
    "## Get PT4 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1050ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates two dictionaries, one with positive instances for each class\n",
    "#and one for negative instances for each class\n",
    "def makePT4Labels(allClassLabels):\n",
    "    PT4LabelsPos = {}\n",
    "    PT4LabelsNeg = {}\n",
    "    \n",
    "    for i in range(len(allClassLabels)):\n",
    "        PT4LabelsPos[i] = []\n",
    "        PT4LabelsNeg[i] = []\n",
    "\n",
    "    for row in allClassLabels:\n",
    "        label_idx = 0\n",
    "        for label in row:\n",
    "            if label == '0':\n",
    "                PT4LabelsPos[label_idx].append(0)\n",
    "                PT4LabelsNeg[label_idx].append(1)\n",
    "            if label == '1':\n",
    "                PT4LabelsPos[label_idx].append(1)\n",
    "                PT4LabelsNeg[label_idx].append(0)\n",
    "            label_idx += 1\n",
    "    assert len(PT4LabelsPos[0]) == len(PT4LabelsNeg[0]) == len(allClassLabels)\n",
    "\n",
    "    return PT4LabelsPos, PT4LabelsNeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "42227642",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT4LabelsPos, PT4LabelsNeg = makePT4Labels(labels_starters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0ebdc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>', '<PRO>', '<CON>']\n",
    "vocab = sorted(set([str(w) for ws in list(x_train_trimmed) + [SPECIAL_TOKENS] for w in ws]))\n",
    "embeddings_path = '../glove.twitter.27B.200d.txt'\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            \n",
    "            if word in vocab:\n",
    "                word2i[word] = i\n",
    "                i += 1\n",
    "                w_weights = [float(i) for i in weights]\n",
    "                vectors.append(w_weights)\n",
    "\n",
    "        vectors = torch.FloatTensor(vectors)\n",
    "\n",
    "    return word2i, vectors\n",
    "\n",
    "def get_oovs(vocab, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = set(vocab) - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def initialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    #Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    return torch.FloatTensor(np.random.normal(0, dim**-0.5, size=(num_embeddings, dim)))\n",
    "    \n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    #Add the oov words to the dict, assigning a new index to each\n",
    "        i = len(glove_embeddings)\n",
    "        for w in oovs:\n",
    "            glove_word2i[w] = i\n",
    "            i +=1\n",
    "    #Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\n",
    "        new_emb = initialize_new_embedding_weights(len(oovs), len(glove_embeddings[0]))\n",
    "        cat_emb = torch.cat((glove_embeddings, new_emb), 0)\n",
    "        return (glove_word2i, cat_emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e4414327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from ../glove.twitter.27B.200d.txt...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../glove.twitter.27B.200d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\John\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m glove_word2i, glove_embeddings \u001b[39m=\u001b[39m read_pretrained_embeddings(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     embeddings_path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     vocab\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m oovs \u001b[39m=\u001b[39m get_oovs(vocab, glove_word2i)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Add the oovs from training data to the word2i encoding, and as new rows\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# to the embeddings matrix\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\John\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 19\u001b[0m in \u001b[0;36mread_pretrained_embeddings\u001b[1;34m(embeddings_path, vocab)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m vectors \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReading embeddings from \u001b[39m\u001b[39m{\u001b[39;00membeddings_path\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(embeddings_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/John/NLP/NLP_sharedTask/SharedTaskModel.ipynb#X61sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../glove.twitter.27B.200d.txt'"
     ]
    }
   ],
   "source": [
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "    embeddings_path,\n",
    "    vocab\n",
    ")\n",
    "oovs = get_oovs(vocab, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54826dfe",
   "metadata": {},
   "source": [
    "Batch Training arguments and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "45e93ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def make_batches(sequences: List[List[str]], labels: List[List[int]], batch_size: int) -> (List[List[List[str]]], List[List[List[int]]]):\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    \n",
    "    num_batch = math.floor(len(sequences)/batch_size)\n",
    "    batched_sents = []\n",
    "    batched_labs = []\n",
    "    \n",
    "    df = pd.DataFrame(data = {\"seq\": sequences, \"lab\": labels})\n",
    "    for i in range(num_batch):\n",
    "        batch = df.sample(n=batch_size)\n",
    "        #print(\"Batch size: \",batch.shape[0])\n",
    "        this_batch_sents = []\n",
    "        this_batch_labs = []\n",
    "        for index, row in batch.iterrows():\n",
    "            sent = row['seq']\n",
    "            label = row['lab']\n",
    "            #df = df[df.seq != sent]\n",
    "            this_batch_sents.append(sent)\n",
    "            this_batch_labs.append(label)\n",
    "        df = df.drop(batch.index)\n",
    "        batched_sents.append(this_batch_sents)\n",
    "        batched_labs.append(this_batch_labs)\n",
    "        \n",
    "    return batched_sents, batched_labs\n",
    "\n",
    "\n",
    "def pad(sents, labs):\n",
    "    lengths = []\n",
    "    for sent in sents:\n",
    "        lengths.append(len(sent))\n",
    "            \n",
    "    max_length = max(lengths)\n",
    "        \n",
    "    for sent in sents:\n",
    "        n = max_length - len(sent)\n",
    "        for i in range(n):\n",
    "            sent.append(\"\")\n",
    "        \n",
    "    return sents\n",
    "\n",
    "\n",
    "# Set your preferred batch size\n",
    "batch_size = 8\n",
    "\n",
    "# We make batches now and use those.\n",
    "PT3_batches = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "PT3_batched_sents, PT3_batched_labs = make_batches(x_train_trimmed, PT3Labels, batch_size)\n",
    "for batch in PT3_batched_sents:\n",
    "    pad_batch = pad(batch, PT3_batched_labs)\n",
    "    PT3_batches.append(pad_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3e413d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "        # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
    "        # Freeze them if training is too slow. Notice that the learning\n",
    "        # rate should probably be smaller in this case.\n",
    "        # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
    "        # for param in self.bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        # TODO: Add an extra hidden layer in the classifier, projecting\n",
    "        #      from the BERT hidden dimension to hidden size.\n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
    "        # TODO: Add a relu nonlinearity to be used in the forward method\n",
    "        #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: Dict\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the contextualized embedding for each input symbol\n",
    "        # We no longer need an LSTM, since BERT encodes context and \n",
    "        # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
    "        encoded_sequence = self.bert(**symbols)\n",
    "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
    "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        #      and check the returns for the forward method.\n",
    "        self.cls = encoded_sequence.pooler_output\n",
    "        self.output = self.cls[:, None, :]\n",
    "        \n",
    "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "eb5f3880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making predictions at test time\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    logits = model(sents)\n",
    "    return list(torch.argmax(logits, axis=2).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cfcd421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    possible_labels,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        print(\"Working on epoch\", i)\n",
    "        for features, labels in tqdm.tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features).squeeze(1)\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm.tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.numpy()))\n",
    "\n",
    "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels)\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d82775",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tok = tokenize(x_valid)\n",
    "validation_\n",
    "\n",
    "epochs = 10\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.00001\n",
    "hidden_size = 64\n",
    "train_input_batches = encode_text(PT3_batched_sents)\n",
    "train_label_batches = PT3_batched_labs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_model = training_loop(\n",
    "    epochs,\n",
    "    train_input_batches,\n",
    "    train_label_batches,\n",
    "    validation_input_sents,\n",
    "    validation_encoded_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    possible_labels\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ae68a39e85a0e7a3c5ef4f6f61e96b04a2a2d93fcc8a43f052173086bc5063d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
