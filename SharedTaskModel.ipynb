{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a0cac7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7371e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0afd5",
   "metadata": {},
   "source": [
    "Prepping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f257209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"arguments-training.tsv\", 'r', encoding='utf8')\n",
    "arguments = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "print(arguments[0])\n",
    "file = open(\"labels-training.tsv\", 'r', encoding='utf8')\n",
    "labels = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ae5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>']\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize, conjoin strings, and add special tokens, remove item ids from labels\n",
    "import spacy\n",
    "\n",
    "def tokenize(text, labels):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    args = []\n",
    "    labs = []\n",
    "    for arg, lab in zip(text, labels):\n",
    "        if arg[3] == 'in favor of':\n",
    "            sep = ['<PRO>']\n",
    "        else:\n",
    "            sep = ['<CON>']\n",
    "        item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "        args.append(item)\n",
    "        labs.append(lab[1:20])\n",
    "\n",
    "    return args, labs\n",
    "\n",
    "arguments_tok, labels = tokenize(arguments, labels)\n",
    "print(arguments_tok[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebdc111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from glove.twitter.27B.200d.txt...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 97>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m oovs \u001b[38;5;241m=\u001b[39m get_oovs(vocab, glove_word2i)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Add the oovs from training data to the word2i encoding, and as new rows\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# to the embeddings matrix\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m word2i, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_word2i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglove_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moovs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mupdate_embeddings\u001b[1;34m(glove_word2i, glove_embeddings, oovs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     new_emb \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_new_embedding_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moovs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglove_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     cat_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((glove_embeddings, new_emb), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (glove_word2i, cat_emb)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36minitialize_new_embedding_weights\u001b[1;34m(num_embeddings, dim)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    torch.FloatTensor: _description_\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#Initialize a num_embeddings x dim matrix with xiavier initiialization\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m, size\u001b[38;5;241m=\u001b[39m(num_embeddings, dim)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Create vocabulary and get word embeddings\n",
    "\n",
    "#Since we have access to the Twitter embeddings already, I just used these. If there are better ones to use, let me know. \n",
    "# - Maddy\n",
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>', '<PRO>', '<CON>']\n",
    "vocab = sorted(set([str(w) for ws in list(arguments_tok) + [SPECIAL_TOKENS] for w in ws]))\n",
    "embeddings_path = 'glove.twitter.27B.200d.txt'\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            \n",
    "            if word in vocab:\n",
    "                word2i[word] = i\n",
    "                i += 1\n",
    "                w_weights = [float(i) for i in weights]\n",
    "                vectors.append(w_weights)\n",
    "\n",
    "        vectors = torch.FloatTensor(vectors)\n",
    "\n",
    "    return word2i, vectors\n",
    "\n",
    "def get_oovs(vocab, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = set(vocab) - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def initialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    #Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    return torch.FloatTensor(np.random.normal(0, dim**-0.5, size=(num_embeddings, dim)))\n",
    "    \n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    #Add the oov words to the dict, assigning a new index to each\n",
    "        i = len(glove_embeddings)\n",
    "        for w in oovs:\n",
    "            glove_word2i[w] = i\n",
    "            i +=1\n",
    "    #Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\n",
    "        new_emb = initialize_new_embedding_weights(len(oovs), len(glove_embeddings[0]))\n",
    "        cat_emb = torch.cat((glove_embeddings, new_emb), 0)\n",
    "        return (glove_word2i, cat_emb)\n",
    "\n",
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "    embeddings_path,\n",
    "    vocab\n",
    ")\n",
    "oovs = get_oovs(vocab, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9828748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Train and Dev\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataset(texts, labels, percent_train):\n",
    "    \n",
    "    num_texts = len(texts)\n",
    "    df = pd.DataFrame(\n",
    "    {'text': texts,\n",
    "     'labels': labels\n",
    "    })\n",
    "    train = df.sample(frac=0.8)\n",
    "    for txt in train:\n",
    "        dev = df[df.text != txt]\n",
    "    train_texts = list(train.loc[:,\"text\"])\n",
    "    train_labels = list(train.loc[:,\"labels\"])\n",
    "    dev_texts = list(dev.loc[:,\"text\"])\n",
    "    dev_labels = list(dev.loc[:,\"labels\"])\n",
    "    \n",
    "    return train_texts, train_labels, dev_texts, dev_labels\n",
    "\n",
    "train_arguments, train_labels, dev_arguments, dev_labels = split_dataset(arguments_tok, labels, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60532e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_arguments) == len(train_labels)\n",
    "assert len(dev_arguments) == len(dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54826dfe",
   "metadata": {},
   "source": [
    "Batch Training arguments and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e93ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN PROGRESS\n",
    "\n",
    "import math\n",
    "\n",
    "def make_batches(sequences: List[List[str]], labels: List[List[int]], batch_size: int) -> (List[ListList[[str]]], List[List[List[int]]]):\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    \n",
    "    num_batch = math.floor(len(sequences)/batch_size)\n",
    "    batched_sents = []\n",
    "    batched_labs = []\n",
    "    \n",
    "    df = pd.DataFrame(data = {\"seq\": sequences, \"lab\": labels})\n",
    "    for i in range(num_batch):\n",
    "        batch = df.sample(n=batch_size, ignore_index = True)\n",
    "        this_batch_sents = []\n",
    "        this_batch_labs = []\n",
    "        for i in range(0, len(batch)-1):\n",
    "            sent = batch._get_value(i, \"seq\")\n",
    "            label = batch._get_value(i, \"lab\")\n",
    "            df = df[df.seq != sent]\n",
    "            this_batch_sents.append(sent)\n",
    "            this_batch_labs.append(label)\n",
    "        batched_sents.append(this_batch_sents)\n",
    "        batched_labs.append(this_batch_labs)\n",
    "        \n",
    "    return batched_sents, batched_labs\n",
    "\n",
    "def pad()\n",
    "\n",
    "\n",
    "# TODO: Set your preferred batch size\n",
    "batch_size = 8\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# We make batches now and use those.\n",
    "batch_tokenized = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "batched_sents, batched_labs = make_batches(train_arguments, train_labels, batch_size)\n",
    "for batch in batched_sents:\n",
    "    pad_batch = pad(batch)\n",
    "    batch_tokenized.append(tok_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
