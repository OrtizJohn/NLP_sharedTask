{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37a0cac7",
   "metadata": {},
   "source": [
    "# SharedTask Touche23 Human Value Detection\n",
    "\n",
    "## Written by Madeleine Wallace and John Ortiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7371e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import precision, recall, f1_score\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58bc4c22",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d0afd5",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f257209f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01002', 'We should ban human cloning', 'in favor of', 'we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.']\n",
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "['A26004', 'We should end affirmative action', 'against', 'affirmative action helps with employment equity.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data/arguments-training.tsv\", 'r', encoding='utf8')\n",
    "x_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_train[0])\n",
    "\n",
    "file = open(\"data/labels-training.tsv\", 'r', encoding='utf8')\n",
    "y_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "\n",
    "file = open(\"data/arguments-validation.tsv\", 'r', encoding='utf8')\n",
    "x_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_valid[0])\n",
    "file = open(\"data/labels-validation.tsv\", 'r', encoding='utf8')\n",
    "y_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(y_valid[0])\n",
    "file = open(\"data/arguments-test.tsv\", 'r', encoding='utf8')\n",
    "x_test = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9d5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to combine xtrain,x-valid, x-test for embeddings\n",
    "  #  originall: x_train_trimmed - > x_train_recommended\n",
    "  # goal x_train_recommended + x_valid_recommended + x_test_recommended \n",
    "\n",
    "def combineAllDataForEmbeddings(xList):\n",
    "  xObj = []\n",
    "  idList=[]\n",
    "  for xTemp in xList:\n",
    "    for row in xTemp:\n",
    "      if(row[0] not in idList):\n",
    "        idList.append(row[0])\n",
    "        xObj.append(row)\n",
    "        #if(len(xObj)<10):\n",
    "         # print(xObj)\n",
    "\n",
    "  \n",
    "\n",
    "  assert ((len(xList[0])+len(xList[1])+len(xList[2])) ==len(xObj))\n",
    "\n",
    "  return xObj\n",
    "\n",
    "xObj = combineAllDataForEmbeddings([x_train, x_valid,x_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77818e32",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "### Tokenizing all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ae5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize, conjoin strings, and add special tokens, remove item ids from labels\n",
    "def tokenize(text, labels=None):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    args = []\n",
    "    labs = []\n",
    "    if(labels != None):\n",
    "        for arg, lab in zip(text, labels):\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "            labs.append(lab[1:20])\n",
    "    else:\n",
    "        \n",
    "        for arg in text:\n",
    "            #print(\"--- \", arg)\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            #print(\"-- \" , item)\n",
    "            args.append(item)\n",
    "            #print(len(args))\n",
    "    return args, labs\n",
    "    \n",
    "def tokenize_allData(x_train,y_train,x_valid,y_valid,x_test):\n",
    "    x_train, y_train = tokenize(x_train, y_train)\n",
    "    x_valid, y_valid = tokenize(x_valid,y_valid)\n",
    "    x_test, _ = tokenize(x_test)\n",
    "    print(x_train[0], y_train[0])\n",
    "    print(\"x_train size: \",len(x_train),\" - x_train size: \",len(y_train))\n",
    "    print(\"___________________\")\n",
    "    print(x_valid[0], y_valid[0])\n",
    "    print(\"x_valid size: \",len(x_valid),\" - y_valid size: \",len(y_valid))\n",
    "    print(\"_______________\")\n",
    "    print(x_test[0])\n",
    "    print(\"xTest size: \", len(x_test))\n",
    "    return x_train,y_train,x_valid,y_valid,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65097a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', We, should, ban, human, cloning, '<CON>', we, should, ban, human, cloning, as, it, will, only, cause, huge, issues, when, you, have, a, bunch, of, the, same, humans, running, around, all, acting, the, same, ., '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_train size:  5393  - x_train size:  5393\n",
      "___________________\n",
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_valid size:  1896  - y_valid size:  1896\n",
      "_______________\n",
      "['<SOS>', We, should, end, affirmative, action, '<CON>', affirmative, action, helps, with, employment, equity, ., '<EOS>']\n",
      "xTest size:  1576\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid,x_test = tokenize_allData(x_train,y_train,x_valid,y_valid,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09566644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', We, should, ban, human, cloning, '<CON>', we, should, ban, human, cloning, as, it, will, only, cause, huge, issues, when, you, have, a, bunch, of, the, same, humans, running, around, all, acting, the, same, ., '<EOS>']\n",
      "xObj size:  8865\n"
     ]
    }
   ],
   "source": [
    "xObj,_ = tokenize(xObj)\n",
    "print(xObj[0])\n",
    "print(\"xObj size: \", len(xObj))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02699a90",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "### Label Selection (Which labels to use in models )\n",
    "==============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb00999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLabels_NotWanted(labels, labels_wanted):\n",
    "    newLabels=[]\n",
    "    for row in labels:\n",
    "        newRow = []\n",
    "        for i in labels_wanted:\n",
    "            newRow.append(row[i])\n",
    "        assert len(newRow) == len(labels_wanted)\n",
    "        newLabels.append(newRow)\n",
    "    return newLabels\n",
    "def removeAllEmptyLabelRows(text,labels):\n",
    "    newText=[]\n",
    "    newLabels=[]\n",
    "    for i in range(len(text)):\n",
    "        intList = [eval(j) for j in labels[i] ]\n",
    "        if(np.sum(intList)!=0):\n",
    "            newText.append(text[i])\n",
    "            newLabels.append(labels[i])\n",
    "      #else:\n",
    "            #print(labels[i])\n",
    "    return newText,newLabels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2087c5dd",
   "metadata": {},
   "source": [
    "### Creating different labels for training on\n",
    "==============================================  \n",
    "   \n",
    "    - labels_starters recommended by Eval:   Self-direction: action, Achievement, Security: personal, Security: societal, Benevolence: caring, Universalism: concern.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b6877e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_labels = {\n",
    "    \"Self-direction: thought\": 0,\n",
    "    \"Self-direction: action\": 1,\t\n",
    "    \"Stimulation\": 2,\n",
    "    \"Hedonism\": 3,\n",
    "    \"Achievement\": 4,\n",
    "    \"Power: dominance\": 5,\n",
    "    \"Power: resources\": 6,\n",
    "    \"Face\": 7,\t\n",
    "    \"Security: personal\": 8,\n",
    "    \"Security: societal\": 9,\n",
    "    \"Tradition\": 10,\n",
    "    \"Conformity: rules\": 11,\n",
    "    \"Conformity: interpersonal\": 12,\n",
    "    \"Humility\": 13,\t\n",
    "    \"Benevolence: caring\": 14,\n",
    "    \"Benevolence: dependability\": 15,\t\n",
    "    \"Universalism: concern\": 16,\t\n",
    "    \"Universalism: nature\": 17,\t\n",
    "    \"Universalism: tolerance\": 18,\n",
    "    \"Universalism: objectivity\": 19\n",
    "}\n",
    "\n",
    "#________________________Define other label category selections here________________________#\n",
    "recommended_categories = [1, 4, 8, 9, 14, 16]\n",
    "starters_dict = {\n",
    "    \"Self-direction: action\":0,\n",
    "    \"Achievement\": 1,\n",
    "    \"Security: personal\": 2,\n",
    "    \"Security: societal\": 3,\n",
    "    \"Benevolence: caring\": 4,\n",
    "    \"Universalism: concern\": 16\n",
    "}\n",
    "security = [8, 9] \n",
    "security_dict = {\n",
    "    \"Security: personal\": 0,\n",
    "    \"Security: societal\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d844b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Requires two inputs the labels index you want to focus on and dictionary reassigning labels\"\"\"\n",
    "def reduceDF_basedOnLabels(focusName,labelFocus,focudDict, x_train,y_train,x_valid,y_valid):\n",
    "  print(f\"({focusName}) - Original items (xTrain:{len(x_train)} - yTrain:{len(y_train)}) , (xValid:{len(x_valid)} - yValid:{len(y_valid)}):\")\n",
    "  labels = removeLabels_NotWanted(y_train, labelFocus)\n",
    "  x_train_trimmed, labels_train = removeAllEmptyLabelRows(x_train, labels)\n",
    "  labels_valid = removeLabels_NotWanted(y_valid, labelFocus)\n",
    "  x_valid_trimmed, labels__valid = removeAllEmptyLabelRows(x_valid, labels_valid)\n",
    "  print(f\"({focusName}) - Item size based on desired labels (xTrain:{len(x_train_trimmed)} - yTrain:{len(labels_train)}) , (xValid:{len(x_valid_trimmed)} - yValid:{len(labels__valid)}):\")\n",
    "  return x_train_trimmed,labels_train,x_valid_trimmed,labels__valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a61ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Recommended) - Original items (xTrain:5393 - yTrain:5393) , (xValid:1896 - yValid:1896):\n",
      "(Recommended) - Item size based on desired labels (xTrain:4985 - yTrain:4985) , (xValid:1768 - yValid:1768):\n",
      "(Security) - Original items (xTrain:5393 - yTrain:5393) , (xValid:1896 - yValid:1896):\n",
      "(Security) - Item size based on desired labels (xTrain:3164 - yTrain:3164) , (xValid:1109 - yValid:1109):\n"
     ]
    }
   ],
   "source": [
    "x_train_recommended,labels_train_recommended,x_valid_recommended,labels_valid_recommended  = reduceDF_basedOnLabels(\"Recommended\",recommended_categories,starters_dict, x_train,y_train,x_valid,y_valid)\n",
    "x_train_security,labels_train_security,x_valid_security,labels_valid_security  = reduceDF_basedOnLabels(\"Security\",security,security_dict, x_train,y_train,x_valid,y_valid)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073bb90d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "### Setting up Labels with different models\n",
    "\n",
    "\n",
    "##### Functions for making PT3 and PT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc45830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#______________________________________________Function: MakePT3______________________________________________#\n",
    "def makePT3Labels(labels_train,labels_valid):\n",
    "    PT3LabelsDict = {}\n",
    "    PT3Labels_train = []\n",
    "    label=0\n",
    "    for row in labels_train:\n",
    "        row_lab = [key for key, value in PT3LabelsDict.items() if value == row]\n",
    "        if row_lab:\n",
    "            PT3Labels_train.append(row_lab[0])\n",
    "        else:\n",
    "            PT3LabelsDict[label] = row\n",
    "            PT3Labels_train.append(label)\n",
    "            label += 1\n",
    "\n",
    "\n",
    "    print(PT3Labels_train)\n",
    "\n",
    "    PT3LabelsValid = []\n",
    "    label=0\n",
    "    for row in labels_valid:\n",
    "        row_lab = [key for key, value in PT3LabelsDict.items() if value == row]\n",
    "        if row_lab:\n",
    "            PT3LabelsValid.append(row_lab[0])\n",
    "        else:\n",
    "            PT3LabelsDict[label] = row\n",
    "            PT3LabelsValid.append(label)\n",
    "            label += 1\n",
    "\n",
    "    print(\"PT3 ySize- \",len(PT3Labels_train))\n",
    "    print(\"PT3 ySizeValid- \",len(PT3LabelsValid))\n",
    "    print(\"Number of new combination labels:\", len(PT3LabelsDict))\n",
    "    return PT3Labels_train, PT3LabelsValid,PT3LabelsDict\n",
    "\n",
    "#______________________________________________Function: MakePT4______________________________________________#\n",
    "#creates two dictionaries, one with positive instances for each class\n",
    "#and one for negative instances for each class\n",
    "def makePT4Labels(allClassLabels):\n",
    "    PT4LabelsPos = {}\n",
    "    PT4LabelsNeg = {}\n",
    "    \n",
    "    for i in range(len(allClassLabels)):\n",
    "        PT4LabelsPos[i] = []\n",
    "        PT4LabelsNeg[i] = []\n",
    "    for i in range(len(allClassLabels)):\n",
    "        row = allClassLabels[i]\n",
    "\n",
    "        for label in row:\n",
    "            if( label == '0'):\n",
    "                PT4LabelsPos[i].append(0)\n",
    "                PT4LabelsNeg[i].append(1)\n",
    "            elif( label == '1'):\n",
    "                PT4LabelsPos[i].append(1)\n",
    "                PT4LabelsNeg[i].append(0)  \n",
    "\n",
    "    \n",
    "    assert len(PT4LabelsPos[0]) == len(PT4LabelsNeg[0]) == len(allClassLabels[0])\n",
    "    assert len(PT4LabelsPos) == len(PT4LabelsNeg) == len(allClassLabels)\n",
    "    return PT4LabelsPos, PT4LabelsNeg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eac4a03",
   "metadata": {},
   "source": [
    "#### Geting PT3 and PT4 Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0455b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To grab valid pt3  and pt4 data one need both labels_train and labels_valid of whatever focus we are trying \n",
    "One instance could be  labels_security_train, labels_security_valid\n",
    "\n",
    "\"\"\"\n",
    "def createPTS(labels_train,labels_valid):\n",
    "  PT3LabelsTrain, PT3LabelsValid, PT3Dict = makePT3Labels(labels_train,labels_valid)\n",
    "  PT4LabelsPos_train, PT4LabelsNeg_train = makePT4Labels(labels_train)\n",
    "  PT4LabelsPos_valid, PT4LabelsNeg_valid = makePT4Labels(labels_valid)\n",
    "  assert(len(labels_train)== len(PT4LabelsPos_train))\n",
    "  assert(len(labels_train)== len(PT4LabelsNeg_train))\n",
    "  assert(len(labels_valid)== len(PT4LabelsPos_valid))\n",
    "  assert(len(labels_valid)== len(PT4LabelsNeg_valid))\n",
    "  return PT3LabelsTrain, PT3LabelsValid, PT3Dict, PT4LabelsPos_train, PT4LabelsNeg_train ,PT4LabelsPos_valid , PT4LabelsNeg_valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42227642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT3 ySize-  4985\n",
      "PT3 ySizeValid-  1768\n",
      "Number of new combination labels: 63\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Can pick which dataset to run here this will determine how many labels are using\"\"\"\n",
    "#PT3LabelsTrain, PT3LabelsValid, PT3Dict,PT4LabelsPos_train, PT4LabelsNeg_train,PT4LabelsPos_valid, PT4LabelsNeg_valid  = createPTS(y_train,y_valid)\n",
    "PT3LabelsTrain, PT3LabelsValid, PT3Dict,PT4LabelsPos_train, PT4LabelsNeg_train,PT4LabelsPos_valid, PT4LabelsNeg_valid  = createPTS(labels_train_recommended,labels_valid_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "793ee53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PT4LabelsPos_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(PT4LabelsPos.update(PT4LabelsNeg))\n",
    "#PT4LabelsPos.update(PT4LabelsNeg)\n",
    "#PT4LabelsTrain = PT4LabelsPos.copy()\n",
    "#(len(PT4LabelsTrain))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "192d26a3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "### Tokenizing Dataset\n",
    "====================\n",
    "  - Based on train, dev , test combination of all words use xObj which combination of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6141b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', We, should, ban, human, cloning, '<CON>', we, should, ban, human, cloning, as, it, will, only, cause, huge, issues, when, you, have, a, bunch, of, the, same, humans, running, around, all, acting, the, same, ., '<EOS>']\n",
      "['<SOS>', We, should, ban, human, cloning, '<CON>', we, should, ban, human, cloning, as, it, will, only, cause, huge, issues, when, you, have, a, bunch, of, the, same, humans, running, around, all, acting, the, same, ., '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "#sanitry check\n",
    "print(xObj[0])\n",
    "print(x_train_recommended[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f74b8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>', '<PRO>', '<CON>']\n",
    "vocab = sorted(set([str(w) for ws in list(xObj) + [SPECIAL_TOKENS] for w in ws]))\n",
    "embeddings_path = '../glove.twitter.27B.200d.txt'\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            \n",
    "            if word in vocab:\n",
    "                word2i[word] = i\n",
    "                i += 1\n",
    "                w_weights = [float(i) for i in weights]\n",
    "                vectors.append(w_weights)\n",
    "\n",
    "        vectors = torch.FloatTensor(vectors)\n",
    "\n",
    "    return word2i, vectors\n",
    "\n",
    "def get_oovs(vocab, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = set(vocab) - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def initialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    #Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    return torch.FloatTensor(np.random.normal(0, dim**-0.5, size=(num_embeddings, dim)))\n",
    "    \n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    #Add the oov words to the dict, assigning a new index to each\n",
    "        i = len(glove_embeddings)\n",
    "        for w in oovs:\n",
    "            glove_word2i[w] = i\n",
    "            i +=1\n",
    "    #Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\n",
    "        new_emb = initialize_new_embedding_weights(len(oovs), len(glove_embeddings[0]))\n",
    "        cat_emb = torch.cat((glove_embeddings, new_emb), 0)\n",
    "        return (glove_word2i, cat_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "258b1609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from ../glove.twitter.27B.200d.txt...\n"
     ]
    }
   ],
   "source": [
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(embeddings_path,vocab)\n",
    "oovs = get_oovs(vocab, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3288ffb2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "### Make batches for each different dataframe \n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57b53b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining batches here\n",
    "\n",
    "def make_batches(sequences: List[List[str]], labels: List[List[int]], batch_size: int) -> (List[List[List[str]]], List[List[List[int]]]):\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    \n",
    "    num_batch = math.floor(len(sequences)/batch_size)\n",
    "    batched_sents = []\n",
    "    batched_labs = []\n",
    "    \n",
    "    df = pd.DataFrame(data = {\"seq\": sequences, \"lab\": labels})\n",
    "    for i in range(num_batch):\n",
    "        batch = df.sample(n=batch_size)\n",
    "        #print(\"Batch size: \",batch.shape[0])\n",
    "        this_batch_sents = []\n",
    "        this_batch_labs = []\n",
    "        for index, row in batch.iterrows():\n",
    "            sent = row['seq']\n",
    "            label = row['lab']\n",
    "            #df = df[df.seq != sent]\n",
    "            this_batch_sents.append(sent)\n",
    "            this_batch_labs.append(label)\n",
    "        df = df.drop(batch.index)\n",
    "        batched_sents.append(this_batch_sents)\n",
    "        batched_labs.append(this_batch_labs)\n",
    "        \n",
    "    return batched_sents, batched_labs\n",
    "\n",
    "\n",
    "def pad(sents, labs):\n",
    "    lengths = []\n",
    "    for sent in sents:\n",
    "        lengths.append(len(sent))\n",
    "            \n",
    "    max_length = max(lengths)\n",
    "        \n",
    "    for sent in sents:\n",
    "        n = max_length - len(sent)\n",
    "        for i in range(n):\n",
    "            sent.append(\"\")\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3c96f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5393 5393\n",
      "674 674\n",
      "8 8\n",
      "1896 1896\n",
      "237 237\n",
      "8 8\n",
      "4985 4985\n",
      "1768 1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#________PT4_________#\\nprint(len(x_train_recommended),len(PT4LabelsPos_train))\\nprint(len(x_train_recommended[0]),len(PT4LabelsPos_train[0]))\\n#batching train positive\\nPT4_batches_train_pos = []\\nPT4_batched_sents_pos, PT4_batched_labs_pos = make_batches(x_train_recommended, PT4LabelsPos_train, batch_size)\\nfor batch in PT4_batched_sents_pos:\\n    pad_batch = pad(batch, PT4_batched_labs_pos)\\n    PT4_batches_train_pos.append(pad_batch)\\n#batching train negative\\nPT4_batches_train_neg = []\\nPT4_batched_sents_neg, PT4_batched_labs_neg = make_batches(x_train_recommended, PT4LabelsNeg_train, batch_size)\\nfor batch in PT4_batched_sents_neg:\\n    pad_batch = pad(batch, PT4_batched_labs_neg)\\n    PT4_batches_train_neg.append(pad_batch)\\n\\n#batching valid positive\\nPT4_batches_valid_pos = []\\nPT4_batched_valid_sents_pos, PT4_batched_valid_labs_pos = make_batches(x_valid_recommended, PT4LabelsPos_valid, batch_size)\\nfor batch in PT4_batched_valid_sents_pos:\\n    pad_batch = pad(batch, PT4_batched_valid_labs_pos)\\n    PT4_batches_valid_pos.append(pad_batch)\\n\\n#batching valid negative\\nPT4_batches_valid_neg = []\\nPT4_batched_valid_sents_neg, PT4_batched_valid_labs_neg = make_batches(x_valid_recommended, PT4LabelsNeg_valid , batch_size)\\nfor batch in PT4_batched_valid_sents_neg:\\n    pad_batch = pad(batch, PT4_batched_valid_labs_neg)\\n    PT4_batches_valid_neg.append(pad_batch)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your preferred batch size\n",
    "batch_size = 8\n",
    "\n",
    "#_________Normal_______ : output -> batches_x_Train, batched_y_train #\n",
    "batches_x_Train = []\n",
    "print(len(x_train),len(y_train))\n",
    "batched_x_train, batched_y_train = make_batches(x_train,y_train,batch_size)\n",
    "for batch in batched_x_train:\n",
    "    pad_batch = pad(batch, batched_y_train)\n",
    "    batches_x_Train.append(pad_batch)\n",
    "print(len(batched_x_train), len(batched_y_train))\n",
    "print(len(batched_x_train[0]),len(batched_y_train[0])) #should batch size\n",
    "\n",
    "bactches_x_Valid = []\n",
    "print(len(x_valid),len(y_valid))\n",
    "batched_x_valid, batched_y_valid = make_batches(x_valid,y_valid,batch_size)\n",
    "for batch in batched_x_valid:\n",
    "    pad_batch = pad(batch, batched_y_valid)\n",
    "    bactches_x_Valid.append(pad_batch)\n",
    "print(len(batched_x_valid), len(batched_y_valid))\n",
    "print(len(batched_x_valid[0]),len(batched_y_valid[0])) #should batch size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_________PT3________#\n",
    "# We make batches now and use those.\n",
    "PT3_batches_train = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "print(len(x_train_recommended),len(PT3LabelsTrain))\n",
    "PT3_batched_sents, PT3_batched_labs = make_batches(x_train_recommended, PT3LabelsTrain, batch_size)\n",
    "for batch in PT3_batched_sents:\n",
    "    pad_batch = pad(batch, PT3_batched_labs)\n",
    "    PT3_batches_train.append(pad_batch)\n",
    "\n",
    "    \n",
    "print(len(x_valid_recommended),len(PT3LabelsValid))\n",
    "PT3_batches_valid = []\n",
    "PT3_batched_sents_valid, PT3_batched_labs_valid = make_batches(x_valid_recommended, PT3LabelsValid, batch_size)\n",
    "for batch in PT3_batched_sents_valid:\n",
    "    pad_batch = pad(batch, PT3_batched_labs_valid)\n",
    "    PT3_batches_valid.append(pad_batch)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#________PT4_________#\n",
    "print(len(x_train_recommended),len(PT4LabelsPos_train))\n",
    "print(len(x_train_recommended[0]),len(PT4LabelsPos_train[0]))\n",
    "#batching train positive\n",
    "PT4_batches_train_pos = []\n",
    "PT4_batched_sents_pos, PT4_batched_labs_pos = make_batches(x_train_recommended, PT4LabelsPos_train, batch_size)\n",
    "for batch in PT4_batched_sents_pos:\n",
    "    pad_batch = pad(batch, PT4_batched_labs_pos)\n",
    "    PT4_batches_train_pos.append(pad_batch)\n",
    "#batching train negative\n",
    "PT4_batches_train_neg = []\n",
    "PT4_batched_sents_neg, PT4_batched_labs_neg = make_batches(x_train_recommended, PT4LabelsNeg_train, batch_size)\n",
    "for batch in PT4_batched_sents_neg:\n",
    "    pad_batch = pad(batch, PT4_batched_labs_neg)\n",
    "    PT4_batches_train_neg.append(pad_batch)\n",
    "\n",
    "#batching valid positive\n",
    "PT4_batches_valid_pos = []\n",
    "PT4_batched_valid_sents_pos, PT4_batched_valid_labs_pos = make_batches(x_valid_recommended, PT4LabelsPos_valid, batch_size)\n",
    "for batch in PT4_batched_valid_sents_pos:\n",
    "    pad_batch = pad(batch, PT4_batched_valid_labs_pos)\n",
    "    PT4_batches_valid_pos.append(pad_batch)\n",
    "\n",
    "#batching valid negative\n",
    "PT4_batches_valid_neg = []\n",
    "PT4_batched_valid_sents_neg, PT4_batched_valid_labs_neg = make_batches(x_valid_recommended, PT4LabelsNeg_valid , batch_size)\n",
    "for batch in PT4_batched_valid_sents_neg:\n",
    "    pad_batch = pad(batch, PT4_batched_valid_labs_neg)\n",
    "    PT4_batches_valid_neg.append(pad_batch)\"\"\"   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09f5d04f",
   "metadata": {},
   "source": [
    "### Encoding Sentences and Label Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b393149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these functions to encode your batches before you call the train loop.\n",
    "\n",
    "def encode_sentences(batch: List[List[str]], word2i: Dict[str, int]) -> torch.LongTensor:\n",
    "    \"\"\"Encode the tokens in each sentence in the batch with a dictionary\n",
    "\n",
    "    Args:\n",
    "        batch (List[List[str]]): The padded and tokenized batch of sentences.\n",
    "        word2i (Dict[str, int]): The encoding dictionary.\n",
    "\n",
    "    Returns:\n",
    "        torch.LongTensor: The tensor of encoded sentences.\n",
    "    \"\"\"\n",
    "    UNK_IDX = word2i[\"<UNK>\"]\n",
    "    tensors = []\n",
    "    for sent in batch:\n",
    "        tensors.append(torch.LongTensor([word2i.get(w, UNK_IDX) for w in sent]))\n",
    "        \n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([int(l) for l in labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a972795",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## Evaluation Functions \n",
    "==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1da0fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import logical_and, sum as t_sum\n",
    "\n",
    "\n",
    "def precision(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(pred_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def recall(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(true_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def f1_score(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    which_label: int\n",
    "):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    possible_labels: List[int]\n",
    "):\n",
    "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
    "    # Macro, so we take the uniform avg.\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7105a79",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "315f9d60",
   "metadata": {},
   "source": [
    "## Model Evaluations Section\n",
    "---\n",
    "### Map of Models\n",
    "  - Base Model NN with embeddings and LSTM \n",
    "  - SVM Model\n",
    "  - Knn Model \n",
    "  - Multi-Class Bert NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a5b824f",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "333fb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ValuesClassifier(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "    output_size: int, \n",
    "    hidden_size: int,\n",
    "    embeddings_tensor: torch.FloatTensor,\n",
    "    pad_idx: int,\n",
    "    dropout_val: float = 0.3,\n",
    "    input_dim: int = 200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, freeze = False, padding_idx = pad_idx)\n",
    "        self.dropout_val = dropout_val\n",
    "        self.dropout_layer = torch.nn.Dropout(p=self.dropout_val, inplace=False)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            self.input_dim,\n",
    "            self.hidden_size,\n",
    "            num_layers=3,\n",
    "            dropout=dropout_val,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (torch.Tensor): The batch size x sequence length tensor of input tokens\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the embedding for each input symbol\n",
    "        embedded = self.embeddings(symbols)\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        # Packs embedded source symbols into a PackedSequence.\n",
    "        # This is an optimization when using padded sequences with an LSTM\n",
    "        lens = (symbols != self.pad_idx).sum(dim=1).to(\"cpu\")\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # -> batch_size x seq_len x encoder_dim, (h0, c0).\n",
    "        packed_outs, (H, C) = self.lstm(packed)\n",
    "        encoded, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_idx,\n",
    "            total_length=None,\n",
    "        )\n",
    "        # Now we have the representation of eahc token encoded by the LSTM.\n",
    "        encoded, (H, C) = self.lstm(embedded)\n",
    "        \n",
    "        # This part looks tricky. All we are doing is getting a tensor\n",
    "        # That indexes the last non-PAD position in each tensor in the batch.\n",
    "        last_enc_out_idxs = lens - 1\n",
    "        # -> B x 1 x 1.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.view([encoded.size(0)] + [1, 1])\n",
    "        # -> 1 x 1 x encoder_dim. This indexes the last non-padded dimension.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.expand(\n",
    "            [-1, -1, encoded.size(-1)]\n",
    "        )\n",
    "        # Get the final hidden state in the LSTM\n",
    "        last_hidden = torch.gather(encoded, 1, last_enc_out_idxs)\n",
    "        return last_hidden\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)\n",
    "        \n",
    "# For making predictions at test time\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    logits = model(sents)\n",
    "    return list(torch.argmax(logits, axis=2).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42076322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_epochs,train_features,train_labels,dev_sents,dev_labels,optimizer,model,possible_labels,):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        print(\"Working on epoch\", i)\n",
    "        for features, labels in tqdm.tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features).squeeze(1)\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm.tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.numpy()))\n",
    "\n",
    "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels)\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5552ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runValueClassifier(batched_x_train,batched_y_train,batched_x_valid,batched_y_valid):\n",
    "    # You can increase epochs if need be\n",
    "    epochs = 10\n",
    "    # TODO: Find a good learning rate\n",
    "    LR = 0.00025\n",
    "    hidden_size = 256\n",
    "    batch_size = 8\n",
    "\n",
    "    #encode\n",
    "    print(len(batch))\n",
    "    train_input_batches = [encode_sentences(batch, word2i) for batch in batched_x_train]\n",
    "    train_label_batches = [encode_labels(batch) for batch in batched_y_train]\n",
    "\n",
    "    validation_input_sents = [encode_sentences(batch, word2i) for batch in batched_x_valid]\n",
    "    validation_encoded_labels = [encode_labels(batch) for batch in batched_y_valid]\n",
    "\n",
    "    num_possible_labels = len(PT3Dict)\n",
    "    model = ValuesClassifier(num_possible_labels, hidden_size, embeddings, word2i['<PAD>'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "    possible_labels = PT3Dict.keys()\n",
    "\n",
    "    output_model = training_loop(\n",
    "        epochs,\n",
    "        train_input_batches,\n",
    "        train_label_batches,\n",
    "        validation_input_sents,\n",
    "        validation_encoded_labels,\n",
    "        optimizer,\n",
    "        model,\n",
    "        possible_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24e2e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "121\n",
      "121\n",
      "121\n",
      "113\n",
      "121\n",
      "109\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "  print(len(batched_x_valid[0][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79209613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [158] at entry 0 and [151] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m runValueClassifier(batched_x_train,batched_y_train,batched_x_valid,batched_y_valid)\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 42\u001b[0m in \u001b[0;36mrunValueClassifier\u001b[1;34m(batched_x_train, batched_y_train, batched_x_valid, batched_y_valid)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#encode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(batch))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_input_batches \u001b[39m=\u001b[39m [encode_sentences(batch, word2i) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batched_x_train]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_label_batches \u001b[39m=\u001b[39m [encode_labels(batch) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batched_y_train]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m validation_input_sents \u001b[39m=\u001b[39m [encode_sentences(batch, word2i) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batched_x_valid]\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 42\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#encode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(batch))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_input_batches \u001b[39m=\u001b[39m [encode_sentences(batch, word2i) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batched_x_train]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_label_batches \u001b[39m=\u001b[39m [encode_labels(batch) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batched_y_train]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m validation_input_sents \u001b[39m=\u001b[39m [encode_sentences(batch, word2i) \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batched_x_valid]\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 42\u001b[0m in \u001b[0;36mencode_sentences\u001b[1;34m(batch, word2i)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     tensors\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mLongTensor([word2i\u001b[39m.\u001b[39mget(w, UNK_IDX) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m sent]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y102sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [158] at entry 0 and [151] at entry 3"
     ]
    }
   ],
   "source": [
    "runValueClassifier(batched_x_train,batched_y_train,batched_x_valid,batched_y_valid)\n",
    "#runValueClassifier(PT3_batched_sents,PT3_batched_labs,PT3_batched_sents_valid,PT3_batched_labs_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb44184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn base have not finished yet\n",
    "\"\"\"ML-kNN (Zhang & Zhou, 2005) is an adaptation of the kNN lazy learning algorithm for multi-label\n",
    "data. Actually this method follows the paradigm of PT4. In essence, ML-kNN uses the kNN algorithm\n",
    "independently for each label l: It finds the k nearest examples to the test instance and considers those\n",
    "that are labelled at least with l as positive and the rest as negative. What mainly differentiates this\n",
    "method from the application of the original kNN algorithm to the transformed problem using PT4 is\n",
    "the use of prior probabilities. ML-kNN has also the capability of producing a ranking of the labels as\n",
    "an output. \n",
    "\n",
    "\n",
    "Luo and Zincir-Heywood (2005) present two systems for multi-label document classification, which\n",
    "are also based on the kNN classifier. The main contribution of their work is on the pre-processing\n",
    "stage for the effective representation of documents. For the classification of a new instance, the\n",
    "systems initially find the k nearest examples. Then for every appearance of each label in each of these\n",
    "examples, they increase a corresponding counter for that label. Finally they output the N labels with\n",
    "the largest counts. N is chosen based on the number of labels of the instance. This is an inappropriate\n",
    "strategy for real-world use, where the number of labels of a new instance is unknown. \"\"\"\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "        # the nearest neighbor classifier simply remembers all the training data\n",
    "        self.Xtr = X\n",
    "        self.ytr = y\n",
    "\n",
    "    def predict(self, X, distance='L1'):\n",
    "        \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        # lets make sure that the output type matches the input type\n",
    "        Ypred = np.zeros(num_test, dtype=self.ytr.dtype)\n",
    "\n",
    "        # loop over all test rows\n",
    "        for i in range(num_test):\n",
    "            # find the nearest training image to the i'th test image\n",
    "            # using the L1 distance (sum of absolute value differences)\n",
    "            if distance == 'L1':\n",
    "                distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1)\n",
    "            # using the L2 distance (sum of absolute value differences)\n",
    "            if distance == 'L2':\n",
    "                distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis=1))\n",
    "            min_index = np.argmin(distances) # get the index with smallest distance\n",
    "            Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63bf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/AnupamMicrosoft/PyTorch-Classification/blob/master/Linear%20Support%20Vector%20Machines.py\n",
    "from torch import nn\n",
    "import random\n",
    "class SVM_Loss(nn.modules.Module):    \n",
    "    def __init__(self):\n",
    "        super(SVM_Loss,self).__init__()\n",
    "    def forward(self, outputs, labels):\n",
    "         return torch.sum(torch.clamp(1 - outputs.t()*labels, min=0))/batch_size\n",
    "\n",
    "        \n",
    "        \n",
    "def runSVM(epochs,input_size,num_classes,train_input_batches, train_label_batches,validation_input_sents,\n",
    "    validation_encoded_labels):      \n",
    "    #SVM regression model and Loss\n",
    "    svm_model = nn.Linear(input_size,num_classes)\n",
    "    #model = LogisticRegression(input_size,num_classes)\n",
    "\n",
    "    ## Loss criteria and SGD optimizer\n",
    "    svm_loss_criteria = SVM_Loss()\n",
    "    #loss_criteria = nn.CrossEntropyLoss()  \n",
    "\n",
    "    #svm_optimizer = torch.optim.SGD(svm_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW(svm_model.parameters(), LR)\n",
    "    \n",
    "    batches = list(zip(train_input_batches, train_label_batches))\n",
    "    random.shuffle(batches)\n",
    "    \n",
    "    \n",
    "    #total_step = len(batches)\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss_epoch = 0\n",
    "        batch_loss = 0\n",
    "        total_batches = 0\n",
    "        for features, labels in tqdm.tqdm(batches):\n",
    "            # Reshape images to (batch_size, input_size)\n",
    "            #images = images.reshape(-1, 28*28)                      \n",
    "            #labels = Variable(2*(labels.float()-0.5))\n",
    "\n",
    "            # Forward pass        \n",
    "            outputs = svm_model(features)           \n",
    "            loss_svm = svm_loss_criteria(outputs, labels)    \n",
    "\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss_svm.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "            #print(\"Model's parameter after the update:\")\n",
    "            #for param2 in svm_model.parameters():\n",
    "             #   print(param2)\n",
    "            total_batches += 1     \n",
    "            batch_loss += loss_svm.item()\n",
    "\n",
    "        avg_loss_epoch = batch_loss/total_batches\n",
    "        print ('Epoch [{}/{}], Averge Loss:for epoch[{}, {:.4f}]' \n",
    "                       .format(epoch+1, num_epochs, epoch+1, avg_loss_epoch ))\n",
    "    return svm_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________pt4 on svm  ______________#\n",
    "# You can increase epochs if need be\n",
    "epochs = 10\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.00001\n",
    "hidden_size = 256\n",
    "batch_size = 8\n",
    "\n",
    "#encode\n",
    "train_input_batches = [encode_sentences(batch, word2i) for batch in PT4_batched_sents]\n",
    "train_label_batches = [encode_labels(batch) for batch in PT4_batched_labs]\n",
    "\n",
    "validation_input_sents = [encode_sentences(batch, word2i) for batch in PT4_batched_sents_valid]\n",
    "validation_encoded_labels = [encode_labels(batch) for batch in PT4_batched_labs_valid]\n",
    "\n",
    "num_possible_labels = len(PT4Dict)\n",
    "#model = ValuesClassifier(num_possible_labels, hidden_size, embeddings, word2i['<PAD>'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "possible_labels = PT4Dict.keys()\n",
    "input_size, _ = getSizeOfPT_Batched(train_input_batches,train_label_batches)\n",
    "\n",
    "runSVM(epochs,input_size,len(possible_labels),train_input_batches, train_label_batches,validation_input_sents,\n",
    "    validation_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab431980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "#_____________PT3 on SVM_________#\n",
    "epochs = 10\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.00001\n",
    "hidden_size = 256\n",
    "batch_size = 8\n",
    "\n",
    "#encode\n",
    "train_input_batches = [encode_sentences(batch, word2i) for batch in PT3_batched_sents]\n",
    "train_label_batches = [encode_labels(batch) for batch in PT3_batched_labs]\n",
    "\n",
    "validation_input_sents = [encode_sentences(batch, word2i) for batch in PT3_batched_sents_valid]\n",
    "validation_encoded_labels = [encode_labels(batch) for batch in PT3_batched_labs_valid]\n",
    "\n",
    "num_possible_labels = len(PT3Dict)\n",
    "#model = ValuesClassifier(num_possible_labels, hidden_size, embeddings, word2i['<PAD>'])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "possible_labels = PT3Dict.keys()\n",
    "runSVM(epochs,len(train_input_batches),len(possible_labels),train_input_batches, train_label_batches,validation_input_sents,\n",
    "    validation_encoded_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd5adaff",
   "metadata": {},
   "source": [
    "### Bert Multi-Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f94ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79596216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "def training_loop_transformer(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    possible_labels,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        print(\"Working on epoch\", i)\n",
    "        for features, labels in tqdm.tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(train_labels, )\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm.tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.numpy()))\n",
    "\n",
    "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels)\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0815284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BERTClass()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0d58980b9c2c2fc9eb8f238783a7a44571002ca4780a7a01628a34cd908c40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
