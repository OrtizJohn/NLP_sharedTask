{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a0cac7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7371e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0afd5",
   "metadata": {},
   "source": [
    "Prepping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f257209f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "['A26004', 'We should end affirmative action', 'against', 'affirmative action helps with employment equity.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data/arguments-training.tsv\", 'r', encoding='utf8')\n",
    "x_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_train[0])\n",
    "\n",
    "file = open(\"data/labels-training.tsv\", 'r', encoding='utf8')\n",
    "y_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "\n",
    "file = open(\"data/arguments-validation.tsv\", 'r', encoding='utf8')\n",
    "x_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_valid[0])\n",
    "file = open(\"data/labels-validation.tsv\", 'r', encoding='utf8')\n",
    "y_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(y_valid[0])\n",
    "file = open(\"data/arguments-test.tsv\", 'r', encoding='utf8')\n",
    "x_test = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ae5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_train size:  5220  - x_train size:  5220\n",
      "___________________\n",
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_valid size:  1896  - y_valid size:  1896\n",
      "_______________\n",
      "['<SOS>', We, should, end, affirmative, action, '<CON>', affirmative, action, helps, with, employment, equity, ., '<EOS>']\n",
      "xTest size:  1576\n"
     ]
    }
   ],
   "source": [
    "#Tokenize, conjoin strings, and add special tokens, remove item ids from labels\n",
    "import spacy\n",
    "\n",
    "def tokenize(text, labels=None):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    args = []\n",
    "    labs = []\n",
    "    if(labels != None):\n",
    "        for arg, lab in zip(text, labels):\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "            labs.append(lab[1:20])\n",
    "    else:\n",
    "        for arg in text:\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "\n",
    "    return args, labs\n",
    "    \n",
    "def tokenize_allData(x_train,y_train,x_valid,y_valid,x_test):\n",
    "    x_train, y_train = tokenize(x_train, y_train)\n",
    "    x_valid, y_valid = tokenize(x_valid,y_valid)\n",
    "    x_test, _ = tokenize(x_test)\n",
    "    print(x_train[0], y_train[0])\n",
    "    print(\"x_train size: \",len(x_train),\" - x_train size: \",len(y_train))\n",
    "    print(\"___________________\")\n",
    "    print(x_valid[0], y_valid[0])\n",
    "    print(\"x_valid size: \",len(x_valid),\" - y_valid size: \",len(y_valid))\n",
    "    print(\"_______________\")\n",
    "    print(x_test[0])\n",
    "    print(\"xTest size: \", len(x_test))\n",
    "    return x_train,y_train,x_valid,y_valid,x_test\n",
    "x_train,y_train,x_valid,y_valid,x_test = tokenize_allData(x_train,y_train,x_valid,y_valid,x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02699a90",
   "metadata": {},
   "source": [
    "## Label Selection (Which labels to use in models )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb00999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLabels_NotWanted(labels, labels_wanted):\n",
    "    newLabels=[]\n",
    "    for row in labels:\n",
    "        newRow = []\n",
    "        for i in labels_wanted:\n",
    "            newRow.append(row[i])\n",
    "        assert len(newRow) == len(labels_wanted)\n",
    "        newLabels.append(newRow)\n",
    "    return newLabels\n",
    "def removeAllEmptyLabelRows(text,labels):\n",
    "    newText=[]\n",
    "    newLabels=[]\n",
    "    for i in range(len(text)):\n",
    "        intList = [eval(j) for j in labels[i] ]\n",
    "        if(np.sum(intList)!=0):\n",
    "            newText.append(text[i])\n",
    "            newLabels.append(labels[i])\n",
    "      #else:\n",
    "            #print(labels[i])\n",
    "    return newText,newLabels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2087c5dd",
   "metadata": {},
   "source": [
    "### Creating different labels for training on     \n",
    "    labels_starters recommended by Eval:   Self-direction: action, Achievement, Security: personal, Security: societal, Benevolence: caring, Universalism: concern.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6877e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original items: 5220\n",
      "Number of items with one of the desired labels: 4835\n"
     ]
    }
   ],
   "source": [
    "reference_labels = {\n",
    "    \"Self-direction: thought\": 0,\n",
    "    \"Self-direction: action\": 1,\t\n",
    "    \"Stimulation\": 2,\n",
    "    \"Hedonism\": 3,\n",
    "    \"Achievement\": 4,\n",
    "    \"Power: dominance\": 5,\n",
    "    \"Power: resources\": 6,\n",
    "    \"Face\": 7,\t\n",
    "    \"Security: personal\": 8,\n",
    "    \"Security: societal\": 9,\n",
    "    \"Tradition\": 10,\n",
    "    \"Conformity: rules\": 11,\n",
    "    \"Conformity: interpersonal\": 12,\n",
    "    \"Humility\": 13,\t\n",
    "    \"Benevolence: caring\": 14,\n",
    "    \"Benevolence: dependability\": 15,\t\n",
    "    \"Universalism: concern\": 16,\t\n",
    "    \"Universalism: nature\": 17,\t\n",
    "    \"Universalism: tolerance\": 18,\n",
    "    \"Universalism: objectivity\": 19\n",
    "}\n",
    "\n",
    "recommended_categories = [1, 4, 8, 9, 14, 16]\n",
    "print(\"Number of original items:\", len(y_train))\n",
    "labels_starters = removeLabels_NotWanted(y_train, recommended_categories)\n",
    "x_train_trimmed, labels_starters_train = removeAllEmptyLabelRows(x_train, labels_starters)\n",
    "print(\"Number of items with one of the desired labels:\", len(labels_starters_train))\n",
    "labels_starters_valid = removeLabels_NotWanted(y_valid, recommended_categories)\n",
    "x_valid_trimmed, labels_starters_valid = removeAllEmptyLabelRows(x_valid, labels_starters_valid)\n",
    "\n",
    "starters_dict = {\n",
    "    \"Self-direction: action\":0,\n",
    "    \"Achievement\": 1,\n",
    "    \"Security: personal\": 2,\n",
    "    \"Security: societal\": 3,\n",
    "    \"Benevolence: caring\": 4,\n",
    "    \"Universalism: concern\": 16\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a61ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original items: 5220\n",
      "Number of items with one of the desired labels: 3090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "security = [8, 9]\n",
    "print(\"Number of original items:\", len(y_train))\n",
    "labels_security = removeLabels_NotWanted(y_train, security)\n",
    "x_train_trimmed, labels_security_train = removeAllEmptyLabelRows(x_train, labels_security)\n",
    "print(\"Number of items with one of the desired labels:\", len(labels_security_train))\n",
    "labels_security_valid = removeLabels_NotWanted(y_valid, security)\n",
    "x_valid_trimmed, labels_security_valid = removeAllEmptyLabelRows(x_valid, labels_security_valid)\n",
    "\n",
    "security_dict = {\n",
    "    \"Security: personal\": 0,\n",
    "    \"Security: societal\": 1\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073bb90d",
   "metadata": {},
   "source": [
    "## Create P3 label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bc45830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makePT3Labels(allClassLabels):\n",
    "    PT3LabelsDict = {}\n",
    "    PT3Labels_idx = []\n",
    "    PT3Labels_vects = []\n",
    "    label=0\n",
    "    for row in allClassLabels:\n",
    "        row_lab = [key for key, value in PT3LabelsDict.items() if value == row]\n",
    "        if row_lab:\n",
    "            PT3Labels_idx.append(row_lab[0])\n",
    "        else:\n",
    "            PT3LabelsDict[label] = row\n",
    "            PT3Labels_idx.append(label)\n",
    "            label += 1\n",
    "    # for row in PT3Labels_idx:\n",
    "    #     label_vect = [0]*len(PT3LabelsDict)\n",
    "    #     label_vect[row] = 1\n",
    "    #     PT3Labels_vects.append(label_vect)\n",
    "\n",
    "    return PT3Labels_idx, PT3LabelsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64791c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new combination labels: 61\n"
     ]
    }
   ],
   "source": [
    "PT3LabelsTrain, PT3Dict = makePT3Labels(labels_starters_train)\n",
    "\n",
    "\n",
    "#PT3Labels_idx = []\n",
    "PT3LabelsValid = []\n",
    "label=0\n",
    "for row in labels_starters_valid:\n",
    "    row_lab = [key for key, value in PT3Dict.items() if value == row]\n",
    "    if row_lab:\n",
    "        PT3LabelsValid.append(row_lab[0])\n",
    "    else:\n",
    "        PT3Dict[label] = row\n",
    "        PT3LabelsValid.append(label)\n",
    "        label += 1\n",
    "# for row in PT3Labels_idx:\n",
    "#     label_vect = [0]*len(PT3Dict)\n",
    "#     label_vect[row] = 1\n",
    "#     PT3LabelsValid.append(label_vect)\n",
    "\n",
    "print(\"Number of new combination labels:\", len(PT3Dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e2edb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new combination labels: 3\n"
     ]
    }
   ],
   "source": [
    "PT3LabelsTrain, PT3Dict = makePT3Labels(labels_security_train)\n",
    "\n",
    "\n",
    "#PT3Labels_idx = []\n",
    "PT3LabelsValid = []\n",
    "label=0\n",
    "for row in labels_security_valid:\n",
    "    row_lab = [key for key, value in PT3Dict.items() if value == row]\n",
    "    if row_lab:\n",
    "        PT3LabelsValid.append(row_lab[0])\n",
    "    else:\n",
    "        PT3Dict[label] = row\n",
    "        PT3LabelsValid.append(label)\n",
    "        label += 1\n",
    "# for row in PT3Labels_idx:\n",
    "#     label_vect = [0]*len(PT3Dict)\n",
    "#     label_vect[row] = 1\n",
    "#     PT3LabelsValid.append(label_vect)\n",
    "\n",
    "print(\"Number of new combination labels:\", len(PT3Dict))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eac4a03",
   "metadata": {},
   "source": [
    "## Get PT4 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1050ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates two dictionaries, one with positive instances for each class\n",
    "#and one for negative instances for each class\n",
    "def makePT4Labels(allClassLabels):\n",
    "    PT4LabelsPos = {}\n",
    "    PT4LabelsNeg = {}\n",
    "    \n",
    "    for i in range(len(allClassLabels)):\n",
    "        PT4LabelsPos[i] = []\n",
    "        PT4LabelsNeg[i] = []\n",
    "\n",
    "    for row in allClassLabels:\n",
    "        label_idx = 0\n",
    "        for label in row:\n",
    "            if label == '0':\n",
    "                PT4LabelsPos[label_idx].append(0)\n",
    "                PT4LabelsNeg[label_idx].append(1)\n",
    "            if label == '1':\n",
    "                PT4LabelsPos[label_idx].append(1)\n",
    "                PT4LabelsNeg[label_idx].append(0)\n",
    "            label_idx += 1\n",
    "    assert len(PT4LabelsPos[0]) == len(PT4LabelsNeg[0]) == len(allClassLabels)\n",
    "\n",
    "    return PT4LabelsPos, PT4LabelsNeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42227642",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT4LabelsPos, PT4LabelsNeg = makePT4Labels(labels_starters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ebdc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>', '<PRO>', '<CON>']\n",
    "vocab = sorted(set([str(w) for ws in list(x_train_trimmed) + [SPECIAL_TOKENS] for w in ws]))\n",
    "embeddings_path = 'glove.twitter.27B.200d.txt'\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            \n",
    "            if word in vocab:\n",
    "                word2i[word] = i\n",
    "                i += 1\n",
    "                w_weights = [float(i) for i in weights]\n",
    "                vectors.append(w_weights)\n",
    "\n",
    "        vectors = torch.FloatTensor(vectors)\n",
    "\n",
    "    return word2i, vectors\n",
    "\n",
    "def get_oovs(vocab, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = set(vocab) - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def initialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    #Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    return torch.FloatTensor(np.random.normal(0, dim**-0.5, size=(num_embeddings, dim)))\n",
    "    \n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    #Add the oov words to the dict, assigning a new index to each\n",
    "        i = len(glove_embeddings)\n",
    "        for w in oovs:\n",
    "            glove_word2i[w] = i\n",
    "            i +=1\n",
    "    #Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\n",
    "        new_emb = initialize_new_embedding_weights(len(oovs), len(glove_embeddings[0]))\n",
    "        cat_emb = torch.cat((glove_embeddings, new_emb), 0)\n",
    "        return (glove_word2i, cat_emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4414327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from glove.twitter.27B.200d.txt...\n"
     ]
    }
   ],
   "source": [
    "# glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "#     embeddings_path,\n",
    "#     vocab\n",
    "# )\n",
    "# oovs = get_oovs(vocab, glove_word2i)\n",
    "\n",
    "# # Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# # to the embeddings matrix\n",
    "# word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54826dfe",
   "metadata": {},
   "source": [
    "Batch Training arguments and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45e93ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def make_batches(sequences: List[List[str]], labels: List[List[int]], batch_size: int) -> (List[List[List[str]]], List[List[List[int]]]):\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    \n",
    "    num_batch = math.floor(len(sequences)/batch_size)\n",
    "    batched_sents = []\n",
    "    batched_labs = []\n",
    "    \n",
    "    df = pd.DataFrame(data = {\"seq\": sequences, \"lab\": labels})\n",
    "    for i in range(num_batch):\n",
    "        batch = df.sample(n=batch_size)\n",
    "        #print(\"Batch size: \",batch.shape[0])\n",
    "        this_batch_sents = []\n",
    "        this_batch_labs = []\n",
    "        for index, row in batch.iterrows():\n",
    "            sent = row['seq']\n",
    "            label = row['lab']\n",
    "            #df = df[df.seq != sent]\n",
    "            this_batch_sents.append(sent)\n",
    "            this_batch_labs.append(label)\n",
    "        df = df.drop(batch.index)\n",
    "        batched_sents.append(this_batch_sents)\n",
    "        batched_labs.append(this_batch_labs)\n",
    "        \n",
    "    return batched_sents, batched_labs\n",
    "\n",
    "\n",
    "def pad(sents, labs):\n",
    "    lengths = []\n",
    "    for sent in sents:\n",
    "        lengths.append(len(sent))\n",
    "            \n",
    "    max_length = max(lengths)\n",
    "        \n",
    "    for sent in sents:\n",
    "        n = max_length - len(sent)\n",
    "        for i in range(n):\n",
    "            sent.append(\"\")\n",
    "        \n",
    "    return sents\n",
    "\n",
    "\n",
    "# Set your preferred batch size\n",
    "batch_size = 8\n",
    "\n",
    "# We make batches now and use those.\n",
    "PT3_batches_train = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "PT3_batched_sents, PT3_batched_labs = make_batches(x_train_trimmed, PT3LabelsTrain, batch_size)\n",
    "for batch in PT3_batched_sents:\n",
    "    pad_batch = pad(batch, PT3_batched_labs)\n",
    "    PT3_batches_train.append(pad_batch)\n",
    "\n",
    "PT3_batches_valid = []\n",
    "PT3_batched_sents_valid, PT3_batched_labs_valid = make_batches(x_valid_trimmed, PT3LabelsValid, batch_size)\n",
    "for batch in PT3_batched_sents_valid:\n",
    "    pad_batch = pad(batch, PT3_batched_labs_valid)\n",
    "    PT3_batches_valid.append(pad_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8455ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Make PT4 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e413d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ValuesClassifier(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "    output_size: int, \n",
    "    hidden_size: int,\n",
    "    embeddings_tensor: torch.FloatTensor,\n",
    "    pad_idx: int,\n",
    "    dropout_val: float = 0.3,\n",
    "    input_dim: int = 200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, freeze = False, padding_idx = pad_idx)\n",
    "        self.dropout_val = dropout_val\n",
    "        self.dropout_layer = torch.nn.Dropout(p=self.dropout_val, inplace=False)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            self.input_dim,\n",
    "            self.hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=dropout_val,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (torch.Tensor): The batch size x sequence length tensor of input tokens\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the embedding for each input symbol\n",
    "        embedded = self.embeddings(symbols)\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        # Packs embedded source symbols into a PackedSequence.\n",
    "        # This is an optimization when using padded sequences with an LSTM\n",
    "        lens = (symbols != self.pad_idx).sum(dim=1).to(\"cpu\")\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # -> batch_size x seq_len x encoder_dim, (h0, c0).\n",
    "        packed_outs, (H, C) = self.lstm(packed)\n",
    "        encoded, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_idx,\n",
    "            total_length=None,\n",
    "        )\n",
    "        # Now we have the representation of eahc token encoded by the LSTM.\n",
    "        encoded, (H, C) = self.lstm(embedded)\n",
    "        \n",
    "        # This part looks tricky. All we are doing is getting a tensor\n",
    "        # That indexes the last non-PAD position in each tensor in the batch.\n",
    "        last_enc_out_idxs = lens - 1\n",
    "        # -> B x 1 x 1.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.view([encoded.size(0)] + [1, 1])\n",
    "        # -> 1 x 1 x encoder_dim. This indexes the last non-padded dimension.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.expand(\n",
    "            [-1, -1, encoded.size(-1)]\n",
    "        )\n",
    "        # Get the final hidden state in the LSTM\n",
    "        last_hidden = torch.gather(encoded, 1, last_enc_out_idxs)\n",
    "        return last_hidden\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb5f3880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making predictions at test time\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    logits = model(sents)\n",
    "    return list(torch.argmax(logits, axis=2).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1da0fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import logical_and, sum as t_sum\n",
    "\n",
    "\n",
    "def precision(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(pred_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def recall(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(true_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def f1_score(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    which_label: int\n",
    "):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    possible_labels: List[int]\n",
    "):\n",
    "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
    "    # Macro, so we take the uniform avg.\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63738d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552ad05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "07f11f32f5b63672a4c0988795b1526caad24c924b40cb7580ccb98e31ed131d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
