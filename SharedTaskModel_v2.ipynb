{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37a0cac7",
   "metadata": {},
   "source": [
    "# SharedTask Touche23 Human Value Detection\n",
    "\n",
    "## Written by Madeleine Wallace and John Ortiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7371e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import precision, recall, f1_score\n",
    "import spacy\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d0afd5",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f257209f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01002', 'We should ban human cloning', 'in favor of', 'we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.']\n",
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "['A26004', 'We should end affirmative action', 'against', 'affirmative action helps with employment equity.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data/arguments-training.tsv\", 'r', encoding='utf8')\n",
    "x_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_train[0])\n",
    "\n",
    "file = open(\"data/labels-training.tsv\", 'r', encoding='utf8')\n",
    "y_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "\n",
    "file = open(\"data/arguments-validation.tsv\", 'r', encoding='utf8')\n",
    "x_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_valid[0])\n",
    "file = open(\"data/labels-validation.tsv\", 'r', encoding='utf8')\n",
    "y_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(y_valid[0])\n",
    "file = open(\"data/arguments-test.tsv\", 'r', encoding='utf8')\n",
    "x_test = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_test[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77818e32",
   "metadata": {},
   "source": [
    "## Tokenizing all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ae5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', We, should, ban, human, cloning, '<CON>', we, should, ban, human, cloning, as, it, will, only, cause, huge, issues, when, you, have, a, bunch, of, the, same, humans, running, around, all, acting, the, same, ., '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_train size:  5393  - x_train size:  5393\n",
      "___________________\n",
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>'] ['0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "x_valid size:  1896  - y_valid size:  1896\n",
      "_______________\n",
      "['<SOS>', We, should, end, affirmative, action, '<CON>', affirmative, action, helps, with, employment, equity, ., '<EOS>']\n",
      "xTest size:  1576\n"
     ]
    }
   ],
   "source": [
    "#Tokenize, conjoin strings, and add special tokens, remove item ids from labels\n",
    "def tokenize(text, labels=None):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    args = []\n",
    "    labs = []\n",
    "    if(labels != None):\n",
    "        for arg, lab in zip(text, labels):\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "            labs.append(lab[1:20])\n",
    "    else:\n",
    "        for arg in text:\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = ['<PRO>']\n",
    "            else:\n",
    "                sep = ['<CON>']\n",
    "            item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "            args.append(item)\n",
    "\n",
    "    return args, labs\n",
    "    \n",
    "def tokenize_allData(x_train,y_train,x_valid,y_valid,x_test):\n",
    "    x_train, y_train = tokenize(x_train, y_train)\n",
    "    x_valid, y_valid = tokenize(x_valid,y_valid)\n",
    "    x_test, _ = tokenize(x_test)\n",
    "    print(x_train[0], y_train[0])\n",
    "    print(\"x_train size: \",len(x_train),\" - x_train size: \",len(y_train))\n",
    "    print(\"___________________\")\n",
    "    print(x_valid[0], y_valid[0])\n",
    "    print(\"x_valid size: \",len(x_valid),\" - y_valid size: \",len(y_valid))\n",
    "    print(\"_______________\")\n",
    "    print(x_test[0])\n",
    "    print(\"xTest size: \", len(x_test))\n",
    "    return x_train,y_train,x_valid,y_valid,x_test\n",
    "x_train,y_train,x_valid,y_valid,x_test = tokenize_allData(x_train,y_train,x_valid,y_valid,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02699a90",
   "metadata": {},
   "source": [
    "## Label Selection (Which labels to use in models )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb00999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLabels_NotWanted(labels, labels_wanted):\n",
    "    newLabels=[]\n",
    "    for row in labels:\n",
    "        newRow = []\n",
    "        for i in labels_wanted:\n",
    "            newRow.append(row[i])\n",
    "        assert len(newRow) == len(labels_wanted)\n",
    "        newLabels.append(newRow)\n",
    "    return newLabels\n",
    "def removeAllEmptyLabelRows(text,labels):\n",
    "    newText=[]\n",
    "    newLabels=[]\n",
    "    for i in range(len(text)):\n",
    "        intList = [eval(j) for j in labels[i] ]\n",
    "        if(np.sum(intList)!=0):\n",
    "            newText.append(text[i])\n",
    "            newLabels.append(labels[i])\n",
    "      #else:\n",
    "            #print(labels[i])\n",
    "    return newText,newLabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087c5dd",
   "metadata": {},
   "source": [
    "### Creating different labels for training on     \n",
    "    labels_starters recommended by Eval:   Self-direction: action, Achievement, Security: personal, Security: societal, Benevolence: caring, Universalism: concern.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b6877e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Number of original items:\",len(x_train), len(y_train))\\nlabels_starters = removeLabels_NotWanted(y_train, recommended_categories)\\nx_train_trimmed, labels_starters_train = removeAllEmptyLabelRows(x_train, labels_starters)\\nprint(\"xTest - yTest\", len(x_train_trimmed),\"-\",len(labels_starters_train))\\nlabels_starters_valid = removeLabels_NotWanted(y_valid, recommended_categories)\\nx_valid_trimmed, labels_starters_valid = removeAllEmptyLabelRows(x_valid, labels_starters_valid)\\nprint(\"xValid - yValid: \", len(x_valid_trimmed),\"-\",len(labels_starters_valid))\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_labels = {\n",
    "    \"Self-direction: thought\": 0,\n",
    "    \"Self-direction: action\": 1,\t\n",
    "    \"Stimulation\": 2,\n",
    "    \"Hedonism\": 3,\n",
    "    \"Achievement\": 4,\n",
    "    \"Power: dominance\": 5,\n",
    "    \"Power: resources\": 6,\n",
    "    \"Face\": 7,\t\n",
    "    \"Security: personal\": 8,\n",
    "    \"Security: societal\": 9,\n",
    "    \"Tradition\": 10,\n",
    "    \"Conformity: rules\": 11,\n",
    "    \"Conformity: interpersonal\": 12,\n",
    "    \"Humility\": 13,\t\n",
    "    \"Benevolence: caring\": 14,\n",
    "    \"Benevolence: dependability\": 15,\t\n",
    "    \"Universalism: concern\": 16,\t\n",
    "    \"Universalism: nature\": 17,\t\n",
    "    \"Universalism: tolerance\": 18,\n",
    "    \"Universalism: objectivity\": 19\n",
    "}\n",
    "\n",
    "#________________________Define other label category selections here________________________#\n",
    "recommended_categories = [1, 4, 8, 9, 14, 16]\n",
    "starters_dict = {\n",
    "    \"Self-direction: action\":0,\n",
    "    \"Achievement\": 1,\n",
    "    \"Security: personal\": 2,\n",
    "    \"Security: societal\": 3,\n",
    "    \"Benevolence: caring\": 4,\n",
    "    \"Universalism: concern\": 16\n",
    "}\n",
    "security = [8, 9] \n",
    "security_dict = {\n",
    "    \"Security: personal\": 0,\n",
    "    \"Security: societal\": 1\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"Number of original items:\",len(x_train), len(y_train))\n",
    "labels_starters = removeLabels_NotWanted(y_train, recommended_categories)\n",
    "x_train_trimmed, labels_starters_train = removeAllEmptyLabelRows(x_train, labels_starters)\n",
    "print(\"xTest - yTest\", len(x_train_trimmed),\"-\",len(labels_starters_train))\n",
    "labels_starters_valid = removeLabels_NotWanted(y_valid, recommended_categories)\n",
    "x_valid_trimmed, labels_starters_valid = removeAllEmptyLabelRows(x_valid, labels_starters_valid)\n",
    "print(\"xValid - yValid: \", len(x_valid_trimmed),\"-\",len(labels_starters_valid))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d844b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Requires two inputs the labels index you want to focus on and dictionary reassigning labels\"\"\"\n",
    "def reduceDF_basedOnLabels(focusName,labelFocus,focudDict, x_train,y_train,x_valid,y_valid):\n",
    "  print(f\"({focusName}) - Original items (xTrain:{len(x_train)} - yTrain:{len(y_train)}) , (xValid:{len(x_valid)} - yValid:{len(y_valid)}):\")\n",
    "  labels = removeLabels_NotWanted(y_train, labelFocus)\n",
    "  x_train_trimmed, labels_train = removeAllEmptyLabelRows(x_train, labels)\n",
    "  labels_valid = removeLabels_NotWanted(y_valid, labelFocus)\n",
    "  x_valid_trimmed, labels__valid = removeAllEmptyLabelRows(x_valid, labels_valid)\n",
    "  print(f\"({focusName}) - Item size based on desired labels (xTrain:{len(x_train_trimmed)} - yTrain:{len(labels_train)}) , (xValid:{len(x_valid_trimmed)} - yValid:{len(labels__valid)}):\")\n",
    "  return x_train_trimmed,labels_train,x_valid_trimmed,labels__valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a61ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Recommended) - Original items (xTrain:5393 - yTrain:5393) , (xValid:1896 - yValid:1896):\n",
      "(Recommended) - Item size based on desired labels (xTrain:4985 - yTrain:4985) , (xValid:1768 - yValid:1768):\n",
      "(Security) - Original items (xTrain:5393 - yTrain:5393) , (xValid:1896 - yValid:1896):\n",
      "(Security) - Item size based on desired labels (xTrain:3164 - yTrain:3164) , (xValid:1109 - yValid:1109):\n"
     ]
    }
   ],
   "source": [
    "x_train_recommended,labels_train_recommended,x_valid_recommended,labels_valid_recommended  = reduceDF_basedOnLabels(\"Recommended\",recommended_categories,starters_dict, x_train,y_train,x_valid,y_valid)\n",
    "x_train_security,labels_train_security,x_valid_security,labels_valid_security  = reduceDF_basedOnLabels(\"Security\",security,security_dict, x_train,y_train,x_valid,y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073bb90d",
   "metadata": {},
   "source": [
    "## Create PT3 Label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc45830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makePT3Labels(labels_train,labels_valid):\n",
    "    PT3LabelsDict = {}\n",
    "    PT3Labels_train = []\n",
    "    #PT3Labels_vects = []\n",
    "    label=0\n",
    "    for row in labels_train:\n",
    "        row_lab = [key for key, value in PT3LabelsDict.items() if value == row]\n",
    "        if row_lab:\n",
    "            PT3Labels_train.append(row_lab[0])\n",
    "        else:\n",
    "            PT3LabelsDict[label] = row\n",
    "            PT3Labels_train.append(label)\n",
    "            label += 1\n",
    "    # for row in PT3Labels_idx:\n",
    "    #     label_vect = [0]*len(PT3LabelsDict)\n",
    "    #     label_vect[row] = 1\n",
    "    #     PT3Labels_vects.append(label_vect)\n",
    "    PT3LabelsValid = []\n",
    "    label=0\n",
    "    for row in labels_valid:\n",
    "        row_lab = [key for key, value in PT3LabelsDict.items() if value == row]\n",
    "        if row_lab:\n",
    "            PT3LabelsValid.append(row_lab[0])\n",
    "        else:\n",
    "            PT3LabelsDict[label] = row\n",
    "            PT3LabelsValid.append(label)\n",
    "            label += 1\n",
    "\n",
    "    print(\"PT3 ySize- \",len(PT3Labels_train))\n",
    "    print(\"PT3 ySizeValid- \",len(PT3LabelsValid))\n",
    "    print(\"Number of new combination labels:\", len(PT3LabelsDict))\n",
    "    return PT3Labels_train, PT3LabelsValid,PT3LabelsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0455b028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT3 ySize-  4985\n",
      "PT3 ySizeValid-  1768\n",
      "Number of new combination labels: 63\n"
     ]
    }
   ],
   "source": [
    "\"\"\"To grab valid pt3 data one need both labels_train and labels_valid of whatever focus we are trying \n",
    "One instance could be  labels_security_train, labels_security_valid\n",
    "\n",
    "\"\"\"\n",
    "PT3LabelsTrain, PT3LabelsValid, PT3Dict = makePT3Labels(labels_train_recommended,labels_valid_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64791c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT3LabelsTrain, PT3Dict = makePT3Labels(labels_starters_train)\n",
    "\n",
    "\n",
    "#PT3Labels_idx = []\n",
    "PT3LabelsValid = []\n",
    "label=0\n",
    "for row in labels_starters_valid:\n",
    "    row_lab = [key for key, value in PT3Dict.items() if value == row]\n",
    "    if row_lab:\n",
    "        PT3LabelsValid.append(row_lab[0])\n",
    "    else:\n",
    "        PT3Dict[label] = row\n",
    "        PT3LabelsValid.append(label)\n",
    "        label += 1\n",
    "# for row in PT3Labels_idx:\n",
    "#     label_vect = [0]*len(PT3Dict)\n",
    "#     label_vect[row] = 1\n",
    "#     PT3LabelsValid.append(label_vect)\n",
    "print(\"PT3 ySize- \",len(PT3LabelsTrain))\n",
    "print(\"PT3 ySizeValid- \",len(PT3LabelsValid))\n",
    "print(\"Number of new combination labels:\", len(PT3Dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2edb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncommment to run PT3 for labels = security\n",
    "\"\"\"PT3LabelsTrain, PT3Dict = makePT3Labels(labels_security_train)\n",
    "\n",
    "\n",
    "#PT3Labels_idx = []\n",
    "PT3LabelsValid = []\n",
    "label=0\n",
    "for row in labels_security_valid:\n",
    "    row_lab = [key for key, value in PT3Dict.items() if value == row]\n",
    "    if row_lab:\n",
    "        PT3LabelsValid.append(row_lab[0])\n",
    "    else:\n",
    "        PT3Dict[label] = row\n",
    "        PT3LabelsValid.append(label)\n",
    "        label += 1\n",
    "# for row in PT3Labels_idx:\n",
    "#     label_vect = [0]*len(PT3Dict)\n",
    "#     label_vect[row] = 1\n",
    "#     PT3LabelsValid.append(label_vect)\n",
    "\n",
    "print(\"Number of new combination labels:\", len(PT3Dict))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac4a03",
   "metadata": {},
   "source": [
    "## Get PT4 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1050ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates two dictionaries, one with positive instances for each class\n",
    "#and one for negative instances for each class\n",
    "def makePT4Labels(allClassLabels):\n",
    "    PT4LabelsPos = {}\n",
    "    PT4LabelsNeg = {}\n",
    "    \n",
    "    for i in range(len(allClassLabels)):\n",
    "        PT4LabelsPos[i] = []\n",
    "        PT4LabelsNeg[i] = []\n",
    "\n",
    "    for row in allClassLabels:\n",
    "        label_idx = 0\n",
    "        for label in row:\n",
    "            if label == '0':\n",
    "                PT4LabelsPos[label_idx].append(0)\n",
    "                PT4LabelsNeg[label_idx].append(1)\n",
    "            if label == '1':\n",
    "                PT4LabelsPos[label_idx].append(1)\n",
    "                PT4LabelsNeg[label_idx].append(0)\n",
    "            label_idx += 1\n",
    "    assert len(PT4LabelsPos[0]) == len(PT4LabelsNeg[0]) == len(allClassLabels)\n",
    "\n",
    "    return PT4LabelsPos, PT4LabelsNeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42227642",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT4LabelsPos, PT4LabelsNeg = makePT4Labels(labels_starters)\n",
    "print(len(PT4LabelsPos), len(PT4LabelsNeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(PT4LabelsPos.update(PT4LabelsNeg))\n",
    "#PT4LabelsPos.update(PT4LabelsNeg)\n",
    "#PT4LabelsTrain = PT4LabelsPos.copy()\n",
    "#(len(PT4LabelsTrain))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "192d26a3",
   "metadata": {},
   "source": [
    "## Tokenizign Dataset\n",
    "\n",
    "  - Based on train, dev , test combination of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6141b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: combine xtrain,x-valid, x-test\n",
    "#  originall: x_train_trimmed - > x_train_recommended\n",
    "# goal x_train_recommended + x_valid_recommended + x_test_recommended "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f74b8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>', '<PRO>', '<CON>']\n",
    "vocab = sorted(set([str(w) for ws in list(x_train_recommended) + [SPECIAL_TOKENS] for w in ws]))\n",
    "embeddings_path = '../glove.twitter.27B.200d.txt'\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            \n",
    "            if word in vocab:\n",
    "                word2i[word] = i\n",
    "                i += 1\n",
    "                w_weights = [float(i) for i in weights]\n",
    "                vectors.append(w_weights)\n",
    "\n",
    "        vectors = torch.FloatTensor(vectors)\n",
    "\n",
    "    return word2i, vectors\n",
    "\n",
    "def get_oovs(vocab, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = set(vocab) - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def initialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    #Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    return torch.FloatTensor(np.random.normal(0, dim**-0.5, size=(num_embeddings, dim)))\n",
    "    \n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    #Add the oov words to the dict, assigning a new index to each\n",
    "        i = len(glove_embeddings)\n",
    "        for w in oovs:\n",
    "            glove_word2i[w] = i\n",
    "            i +=1\n",
    "    #Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\n",
    "        new_emb = initialize_new_embedding_weights(len(oovs), len(glove_embeddings[0]))\n",
    "        cat_emb = torch.cat((glove_embeddings, new_emb), 0)\n",
    "        return (glove_word2i, cat_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "258b1609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from ../glove.twitter.27B.200d.txt...\n"
     ]
    }
   ],
   "source": [
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(embeddings_path,vocab)\n",
    "oovs = get_oovs(vocab, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288ffb2",
   "metadata": {},
   "source": [
    "### Make batches for each different dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b53b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining batches here\n",
    "\n",
    "def make_batches(sequences: List[List[str]], labels: List[List[int]], batch_size: int) -> (List[List[List[str]]], List[List[List[int]]]):\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    \n",
    "    num_batch = math.floor(len(sequences)/batch_size)\n",
    "    batched_sents = []\n",
    "    batched_labs = []\n",
    "    \n",
    "    df = pd.DataFrame(data = {\"seq\": sequences, \"lab\": labels})\n",
    "    for i in range(num_batch):\n",
    "        batch = df.sample(n=batch_size)\n",
    "        #print(\"Batch size: \",batch.shape[0])\n",
    "        this_batch_sents = []\n",
    "        this_batch_labs = []\n",
    "        for index, row in batch.iterrows():\n",
    "            sent = row['seq']\n",
    "            label = row['lab']\n",
    "            #df = df[df.seq != sent]\n",
    "            this_batch_sents.append(sent)\n",
    "            this_batch_labs.append(label)\n",
    "        df = df.drop(batch.index)\n",
    "        batched_sents.append(this_batch_sents)\n",
    "        batched_labs.append(this_batch_labs)\n",
    "        \n",
    "    return batched_sents, batched_labs\n",
    "\n",
    "\n",
    "def pad(sents, labs):\n",
    "    lengths = []\n",
    "    for sent in sents:\n",
    "        lengths.append(len(sent))\n",
    "            \n",
    "    max_length = max(lengths)\n",
    "        \n",
    "    for sent in sents:\n",
    "        n = max_length - len(sent)\n",
    "        for i in range(n):\n",
    "            sent.append(\"\")\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3c96f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4985 4985\n",
      "1768 1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#batching train\\nPT4_batches_train = []\\nPT4_batched_sents, PT4_batched_labs = make_batches(x_train_trimmed, PT4LabelsTrain, batch_size)\\nfor batch in PT4_batched_sents:\\n    pad_batch = pad(batch, PT4_batched_labs_valid)\\n    PT4_batches_train.append(pad_batch)\\n#batching valid\\nPT4_batches_valid = []\\nPT4_batched_sents_valid, PT4_batched_labs_valid = make_batches(x_valid_trimmed, PT4LabelsValid, batch_size)\\nfor batch in PT4_batched_sents_valid:\\n    pad_batch = pad(batch, PT4_batched_labs_valid)\\n    PT4_batches_valid.append(pad_batch)\\n    '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your preferred batch size\n",
    "batch_size = 8\n",
    "\n",
    "#_________PT3________#\n",
    "# We make batches now and use those.\n",
    "PT3_batches_train = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "print(len(x_train_recommended),len(PT3LabelsTrain))\n",
    "PT3_batched_sents, PT3_batched_labs = make_batches(x_train_recommended, PT3LabelsTrain, batch_size)\n",
    "for batch in PT3_batched_sents:\n",
    "    pad_batch = pad(batch, PT3_batched_labs)\n",
    "    PT3_batches_train.append(pad_batch)\n",
    "    \n",
    "print(len(x_valid_recommended),len(PT3LabelsValid))\n",
    "PT3_batches_valid = []\n",
    "PT3_batched_sents_valid, PT3_batched_labs_valid = make_batches(x_valid_recommended, PT3LabelsValid, batch_size)\n",
    "for batch in PT3_batched_sents_valid:\n",
    "    pad_batch = pad(batch, PT3_batched_labs_valid)\n",
    "    PT3_batches_valid.append(pad_batch)\n",
    "   \n",
    "#________PT4_________#\n",
    "\"\"\"\n",
    "#batching train\n",
    "PT4_batches_train = []\n",
    "PT4_batched_sents, PT4_batched_labs = make_batches(x_train_trimmed, PT4LabelsTrain, batch_size)\n",
    "for batch in PT4_batched_sents:\n",
    "    pad_batch = pad(batch, PT4_batched_labs_valid)\n",
    "    PT4_batches_train.append(pad_batch)\n",
    "#batching valid\n",
    "PT4_batches_valid = []\n",
    "PT4_batched_sents_valid, PT4_batched_labs_valid = make_batches(x_valid_trimmed, PT4LabelsValid, batch_size)\n",
    "for batch in PT4_batched_sents_valid:\n",
    "    pad_batch = pad(batch, PT4_batched_labs_valid)\n",
    "    PT4_batches_valid.append(pad_batch)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e413d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ValuesClassifier(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "    output_size: int, \n",
    "    hidden_size: int,\n",
    "    embeddings_tensor: torch.FloatTensor,\n",
    "    pad_idx: int,\n",
    "    dropout_val: float = 0.3,\n",
    "    input_dim: int = 200,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, freeze = False, padding_idx = pad_idx)\n",
    "        self.dropout_val = dropout_val\n",
    "        self.dropout_layer = torch.nn.Dropout(p=self.dropout_val, inplace=False)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            self.input_dim,\n",
    "            self.hidden_size,\n",
    "            num_layers=5,\n",
    "            dropout=dropout_val,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (torch.Tensor): The batch size x sequence length tensor of input tokens\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the embedding for each input symbol\n",
    "        embedded = self.embeddings(symbols)\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        # Packs embedded source symbols into a PackedSequence.\n",
    "        # This is an optimization when using padded sequences with an LSTM\n",
    "        lens = (symbols != self.pad_idx).sum(dim=1).to(\"cpu\")\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # -> batch_size x seq_len x encoder_dim, (h0, c0).\n",
    "        packed_outs, (H, C) = self.lstm(packed)\n",
    "        encoded, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_idx,\n",
    "            total_length=None,\n",
    "        )\n",
    "        # Now we have the representation of eahc token encoded by the LSTM.\n",
    "        encoded, (H, C) = self.lstm(embedded)\n",
    "        \n",
    "        # This part looks tricky. All we are doing is getting a tensor\n",
    "        # That indexes the last non-PAD position in each tensor in the batch.\n",
    "        last_enc_out_idxs = lens - 1\n",
    "        # -> B x 1 x 1.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.view([encoded.size(0)] + [1, 1])\n",
    "        # -> 1 x 1 x encoder_dim. This indexes the last non-padded dimension.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.expand(\n",
    "            [-1, -1, encoded.size(-1)]\n",
    "        )\n",
    "        # Get the final hidden state in the LSTM\n",
    "        last_hidden = torch.gather(encoded, 1, last_enc_out_idxs)\n",
    "        return last_hidden\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)\n",
    "        \n",
    "# For making predictions at test time\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    logits = model(sents)\n",
    "    return list(torch.argmax(logits, axis=2).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b393149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these functions to encode your batches before you call the train loop.\n",
    "\n",
    "def encode_sentences(batch: List[List[str]], word2i: Dict[str, int]) -> torch.LongTensor:\n",
    "    \"\"\"Encode the tokens in each sentence in the batch with a dictionary\n",
    "\n",
    "    Args:\n",
    "        batch (List[List[str]]): The padded and tokenized batch of sentences.\n",
    "        word2i (Dict[str, int]): The encoding dictionary.\n",
    "\n",
    "    Returns:\n",
    "        torch.LongTensor: The tensor of encoded sentences.\n",
    "    \"\"\"\n",
    "    UNK_IDX = word2i[\"<UNK>\"]\n",
    "    tensors = []\n",
    "    for sent in batch:\n",
    "        tensors.append(torch.LongTensor([word2i.get(w, UNK_IDX) for w in sent]))\n",
    "        \n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([int(l) for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da0fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import logical_and, sum as t_sum\n",
    "\n",
    "\n",
    "def precision(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(pred_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def recall(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(true_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def f1_score(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    which_label: int\n",
    "):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    possible_labels: List[int]\n",
    "):\n",
    "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
    "    # Macro, so we take the uniform avg.\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42076322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    possible_labels,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        print(\"Working on epoch\", i)\n",
    "        for features, labels in tqdm.tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features).squeeze(1)\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm.tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.numpy()))\n",
    "\n",
    "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels)\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5552ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Working on epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/623 [00:43<26:01,  2.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), LR)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m possible_labels \u001b[39m=\u001b[39m PT3Dict\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m output_model \u001b[39m=\u001b[39m training_loop(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     train_input_batches,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_label_batches,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     validation_input_sents,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     validation_encoded_labels,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     possible_labels\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel.ipynb Cell 33\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(num_epochs, train_features, train_labels, dev_sents, dev_labels, optimizer, model, possible_labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(preds, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Backpropogate the loss through our model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel.ipynb#Y153sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# You can increase epochs if need be\n",
    "epochs = 10\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.0001\n",
    "hidden_size = 256\n",
    "batch_size = 8\n",
    "\n",
    "#encode\n",
    "train_input_batches = [encode_sentences(batch, word2i) for batch in PT3_batched_sents]\n",
    "train_label_batches = [encode_labels(batch) for batch in PT3_batched_labs]\n",
    "\n",
    "validation_input_sents = [encode_sentences(batch, word2i) for batch in PT3_batched_sents_valid]\n",
    "validation_encoded_labels = [encode_labels(batch) for batch in PT3_batched_labs_valid]\n",
    "\n",
    "num_possible_labels = len(PT3Dict)\n",
    "model = ValuesClassifier(num_possible_labels, hidden_size, embeddings, word2i['<PAD>'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "possible_labels = PT3Dict.keys()\n",
    "\n",
    "output_model = training_loop(\n",
    "    epochs,\n",
    "    train_input_batches,\n",
    "    train_label_batches,\n",
    "    validation_input_sents,\n",
    "    validation_encoded_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    possible_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb44184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn base have not finished yet\n",
    "\"\"\"ML-kNN (Zhang & Zhou, 2005) is an adaptation of the kNN lazy learning algorithm for multi-label\n",
    "data. Actually this method follows the paradigm of PT4. In essence, ML-kNN uses the kNN algorithm\n",
    "independently for each label l: It finds the k nearest examples to the test instance and considers those\n",
    "that are labelled at least with l as positive and the rest as negative. What mainly differentiates this\n",
    "method from the application of the original kNN algorithm to the transformed problem using PT4 is\n",
    "the use of prior probabilities. ML-kNN has also the capability of producing a ranking of the labels as\n",
    "an output. \n",
    "\n",
    "\n",
    "Luo and Zincir-Heywood (2005) present two systems for multi-label document classification, which\n",
    "are also based on the kNN classifier. The main contribution of their work is on the pre-processing\n",
    "stage for the effective representation of documents. For the classification of a new instance, the\n",
    "systems initially find the k nearest examples. Then for every appearance of each label in each of these\n",
    "examples, they increase a corresponding counter for that label. Finally they output the N labels with\n",
    "the largest counts. N is chosen based on the number of labels of the instance. This is an inappropriate\n",
    "strategy for real-world use, where the number of labels of a new instance is unknown. \"\"\"\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "        # the nearest neighbor classifier simply remembers all the training data\n",
    "        self.Xtr = X\n",
    "        self.ytr = y\n",
    "\n",
    "    def predict(self, X, distance='L1'):\n",
    "        \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        # lets make sure that the output type matches the input type\n",
    "        Ypred = np.zeros(num_test, dtype=self.ytr.dtype)\n",
    "\n",
    "        # loop over all test rows\n",
    "        for i in range(num_test):\n",
    "            # find the nearest training image to the i'th test image\n",
    "            # using the L1 distance (sum of absolute value differences)\n",
    "            if distance == 'L1':\n",
    "                distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1)\n",
    "            # using the L2 distance (sum of absolute value differences)\n",
    "            if distance == 'L2':\n",
    "                distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis=1))\n",
    "            min_index = np.argmin(distances) # get the index with smallest distance\n",
    "            Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63bf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/AnupamMicrosoft/PyTorch-Classification/blob/master/Linear%20Support%20Vector%20Machines.py\n",
    "from torch import nn\n",
    "import random\n",
    "class SVM_Loss(nn.modules.Module):    \n",
    "    def __init__(self):\n",
    "        super(SVM_Loss,self).__init__()\n",
    "    def forward(self, outputs, labels):\n",
    "         return torch.sum(torch.clamp(1 - outputs.t()*labels, min=0))/batch_size\n",
    "\n",
    "        \n",
    "        \n",
    "def runSVM(epochs,input_size,num_classes,train_input_batches, train_label_batches,validation_input_sents,\n",
    "    validation_encoded_labels):      \n",
    "    #SVM regression model and Loss\n",
    "    svm_model = nn.Linear(input_size,num_classes)\n",
    "    #model = LogisticRegression(input_size,num_classes)\n",
    "\n",
    "    ## Loss criteria and SGD optimizer\n",
    "    svm_loss_criteria = SVM_Loss()\n",
    "    #loss_criteria = nn.CrossEntropyLoss()  \n",
    "\n",
    "    #svm_optimizer = torch.optim.SGD(svm_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW(svm_model.parameters(), LR)\n",
    "    \n",
    "    batches = list(zip(train_input_batches, train_label_batches))\n",
    "    random.shuffle(batches)\n",
    "    \n",
    "    \n",
    "    #total_step = len(batches)\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss_epoch = 0\n",
    "        batch_loss = 0\n",
    "        total_batches = 0\n",
    "        for features, labels in tqdm.tqdm(batches):\n",
    "            # Reshape images to (batch_size, input_size)\n",
    "            #images = images.reshape(-1, 28*28)                      \n",
    "            #labels = Variable(2*(labels.float()-0.5))\n",
    "\n",
    "            # Forward pass        \n",
    "            outputs = svm_model(features)           \n",
    "            loss_svm = svm_loss_criteria(outputs, labels)    \n",
    "\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss_svm.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "            #print(\"Model's parameter after the update:\")\n",
    "            #for param2 in svm_model.parameters():\n",
    "             #   print(param2)\n",
    "            total_batches += 1     \n",
    "            batch_loss += loss_svm.item()\n",
    "\n",
    "        avg_loss_epoch = batch_loss/total_batches\n",
    "        print ('Epoch [{}/{}], Averge Loss:for epoch[{}, {:.4f}]' \n",
    "                       .format(epoch+1, num_epochs, epoch+1, avg_loss_epoch ))\n",
    "    return svm_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________pt4 on svm  ______________#\n",
    "# You can increase epochs if need be\n",
    "epochs = 10\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.00001\n",
    "hidden_size = 256\n",
    "batch_size = 8\n",
    "\n",
    "#encode\n",
    "train_input_batches = [encode_sentences(batch, word2i) for batch in PT4_batched_sents]\n",
    "train_label_batches = [encode_labels(batch) for batch in PT4_batched_labs]\n",
    "\n",
    "validation_input_sents = [encode_sentences(batch, word2i) for batch in PT4_batched_sents_valid]\n",
    "validation_encoded_labels = [encode_labels(batch) for batch in PT4_batched_labs_valid]\n",
    "\n",
    "num_possible_labels = len(PT4Dict)\n",
    "#model = ValuesClassifier(num_possible_labels, hidden_size, embeddings, word2i['<PAD>'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "possible_labels = PT4Dict.keys()\n",
    "input_size, _ = getSizeOfPT_Batched(train_input_batches,train_label_batches)\n",
    "\n",
    "runSVM(epochs,input_size,len(possible_labels),train_input_batches, train_label_batches,validation_input_sents,\n",
    "    validation_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab431980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "#_____________PT3 on SVM_________#\n",
    "epochs = 10\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.00001\n",
    "hidden_size = 256\n",
    "batch_size = 8\n",
    "\n",
    "#encode\n",
    "train_input_batches = [encode_sentences(batch, word2i) for batch in PT3_batched_sents]\n",
    "train_label_batches = [encode_labels(batch) for batch in PT3_batched_labs]\n",
    "\n",
    "validation_input_sents = [encode_sentences(batch, word2i) for batch in PT3_batched_sents_valid]\n",
    "validation_encoded_labels = [encode_labels(batch) for batch in PT3_batched_labs_valid]\n",
    "\n",
    "num_possible_labels = len(PT3Dict)\n",
    "#model = ValuesClassifier(num_possible_labels, hidden_size, embeddings, word2i['<PAD>'])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "possible_labels = PT3Dict.keys()\n",
    "runSVM(epochs,len(train_input_batches),len(possible_labels),train_input_batches, train_label_batches,validation_input_sents,\n",
    "    validation_encoded_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd5adaff",
   "metadata": {},
   "source": [
    "### Bert Multi-Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6732c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8e0ad0d",
   "metadata": {},
   "source": [
    "Re-Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba9ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01002', 'We should ban human cloning', 'in favor of', 'we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.']\n",
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "['A26004', 'We should end affirmative action', 'against', 'affirmative action helps with employment equity.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data/arguments-training.tsv\", 'r', encoding='utf8')\n",
    "x_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_train[0])\n",
    "\n",
    "file = open(\"data/labels-training.tsv\", 'r', encoding='utf8')\n",
    "y_train = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "\n",
    "file = open(\"data/arguments-validation.tsv\", 'r', encoding='utf8')\n",
    "x_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_valid[0])\n",
    "file = open(\"data/labels-validation.tsv\", 'r', encoding='utf8')\n",
    "y_valid = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(y_valid[0])\n",
    "file = open(\"data/arguments-test.tsv\", 'r', encoding='utf8')\n",
    "x_test = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "file.close()\n",
    "print(x_test[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41515df5",
   "metadata": {},
   "source": [
    "Retokenize for BERT optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cae8bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.args = dataframe.args\n",
    "        self.targets = self.data.labs\n",
    "\n",
    "        special_tokens = [\"<PRO>\", \"<CON>\"]\n",
    "        self.tokenizer.add_tokens(special_tokens, special_tokens = True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.args)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        args = str(self.args[index])\n",
    "        args = \" \".join(args.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            args,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length = len(max(self.args)),\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        self.targets[index] = [int(l) for l in self.targets[index]]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ab9e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "def combine(text, labels=None):\n",
    "    \n",
    "    args = []\n",
    "    labs = []\n",
    "    if(labels != None):\n",
    "        for arg, lab in zip(text, labels):\n",
    "            if arg[3] == 'in favor of':\n",
    "                sep = '<PRO>'\n",
    "            else:\n",
    "                sep = '<CON>'\n",
    "            item = '<SOS>' + arg[1] + sep + arg[3] + '<EOS>'\n",
    "            args.append(item)\n",
    "            labs.append(lab[1:20])\n",
    "    else:\n",
    "        print(\"ValueError: labels can not be noneType Object\")\n",
    "    \n",
    "    combined = pd.DataFrame(data = {'args': args, 'labs': labs})\n",
    "    return combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a05e8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently ['A01002', 'We should ban human cloning', 'in favor of', 'we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.']\n",
      "goal  A01002 We should ban human cloning in favor of we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b3404b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------my guuess where you change to tensor objects: thought process - is any reference call after would be reference its og form \n",
    "train_comb = combine(x_train, y_train)\n",
    "valid_comb = combine(x_valid, y_valid)\n",
    "\n",
    "\n",
    "for lab in train_comb[\"labs\"]:\n",
    "    lab = [int(l) for l in lab]\n",
    "\n",
    "for lab in valid_comb[\"labs\"]:\n",
    "    lab = [int(l) for l in lab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0377799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_set = CustomDataset(train_comb, tokenizer)\n",
    "testing_set = CustomDataset(valid_comb, tokenizer)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59f94ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\modeling_utils.py:367\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    368\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> 713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\serialization.py:930\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    929\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m--> 930\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    932\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\serialization.py:871\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m root_key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m deserialized_objects:\n\u001b[1;32m--> 871\u001b[0m     obj \u001b[39m=\u001b[39m cast(Storage, torch\u001b[39m.\u001b[39;49m_UntypedStorage(nbytes))\n\u001b[0;32m    872\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9437184 bytes.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel_v2.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m cuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m BERTClass()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel_v2.ipynb Cell 50\u001b[0m in \u001b[0;36mBERTClass.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39msuper\u001b[39m(BERTClass, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1 \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mBertModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDropout(\u001b[39m0.3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y112sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear(\u001b[39m768\u001b[39m, \u001b[39m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\modeling_utils.py:2067\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2064\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2066\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2067\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2069\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2072\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2073\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\modeling_utils.py:371\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 371\u001b[0m         \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    372\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    375\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m             )\n\u001b[0;32m    377\u001b[0m         \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 20)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79596216",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0): #------------------------------------TRAIN X CALLED HERE\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0815284",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4005888 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel_v2.ipynb Cell 52\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(epoch)\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel_v2.ipynb Cell 52\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m token_type_ids \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m targets \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(ids, mask, token_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, targets)\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\johno\\NLP\\NLP_sharedTask\\SharedTaskModel_v2.ipynb Cell 52\u001b[0m in \u001b[0;36mBERTClass.forward\u001b[1;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, ids, mask, token_type_ids):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     _, output_1\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1(ids, attention_mask \u001b[39m=\u001b[39;49m mask, token_type_ids \u001b[39m=\u001b[39;49m token_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(output_1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johno/NLP/NLP_sharedTask/SharedTaskModel_v2.ipynb#Y114sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     output_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2(output_1)\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1009\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1011\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1012\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1013\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[1;32m-> 1018\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1019\u001b[0m     embedding_output,\n\u001b[0;32m   1020\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1021\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1022\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1023\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1025\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1029\u001b[0m )\n\u001b[0;32m   1030\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1031\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    598\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    600\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m         hidden_states,\n\u001b[0;32m    609\u001b[0m         attention_mask,\n\u001b[0;32m    610\u001b[0m         layer_head_mask,\n\u001b[0;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    613\u001b[0m         past_key_value,\n\u001b[0;32m    614\u001b[0m         output_attentions,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    491\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    494\u001b[0m         hidden_states,\n\u001b[0;32m    495\u001b[0m         attention_mask,\n\u001b[0;32m    496\u001b[0m         head_mask,\n\u001b[0;32m    497\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    498\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    500\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    502\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    415\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 423\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    424\u001b[0m         hidden_states,\n\u001b[0;32m    425\u001b[0m         attention_mask,\n\u001b[0;32m    426\u001b[0m         head_mask,\n\u001b[0;32m    427\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    428\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    429\u001b[0m         past_key_value,\n\u001b[0;32m    430\u001b[0m         output_attentions,\n\u001b[0;32m    431\u001b[0m     )\n\u001b[0;32m    432\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    433\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\johno\\Anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:361\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[1;32m--> 361\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attention_probs, value_layer)\n\u001b[0;32m    363\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    364\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4005888 bytes."
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0db2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0d58980b9c2c2fc9eb8f238783a7a44571002ca4780a7a01628a34cd908c40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
