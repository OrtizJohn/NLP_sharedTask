{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a0cac7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7371e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "from util import get_snli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0afd5",
   "metadata": {},
   "source": [
    "Prepping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f257209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A01001', 'Entrapment should be legalized', 'in favor of', \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\"]\n",
      "['A01001', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"arguments-training.tsv\", 'r', encoding='utf8')\n",
    "arguments = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "print(arguments[0])\n",
    "file = open(\"labels-training.tsv\", 'r', encoding='utf8')\n",
    "labels = [line.strip().split('\\t') for line in file.readlines()[1:]]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23ae5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', Entrapment, should, be, legalized, '<CON>', if, entrapment, can, serve, to, more, easily, capture, wanted, criminals, ,, then, why, should, n't, it, be, legal, ?, '<EOS>']\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize, conjoin strings, and add special tokens, remove item ids from labels\n",
    "import spacy\n",
    "\n",
    "def tokenize(text, labels):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    args = []\n",
    "    labs = []\n",
    "    for arg, lab in zip(text, labels):\n",
    "        if arg[3] == 'in favor of':\n",
    "            sep = ['<PRO>']\n",
    "        else:\n",
    "            sep = ['<CON>']\n",
    "        item = ['<SOS>'] + list(nlp(arg[1])) + sep + list(nlp(arg[3])) + ['<EOS>']\n",
    "        args.append(item)\n",
    "        labs.append(lab[1:20])\n",
    "\n",
    "    return args, labs\n",
    "\n",
    "arguments_tok, labels = tokenize(arguments, labels)\n",
    "print(arguments_tok[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ebdc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vocabulary and get word embeddings\n",
    "\n",
    "#Since we have access to the Twitter embeddings already, I just used these. If there are better ones to use, let me know. - Maddy\n",
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>', '<PRO>', '<CON>']\n",
    "vocab = sorted(set([str(w) for ws in list(args) + [SPECIAL_TOKENS] for w in ws]))\n",
    "embeddings_path = 'glove.twitter.27B.200d.txt'\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            \n",
    "            if word in vocab:\n",
    "                word2i[word] = i\n",
    "                i += 1\n",
    "                w_weights = [float(i) for i in weights]\n",
    "                vectors.append(w_weights)\n",
    "\n",
    "        vectors = torch.FloatTensor(vectors)\n",
    "\n",
    "    return word2i, vectors\n",
    "\n",
    "def get_oovs(vocab, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = set(vocab) - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def initialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    #Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    return torch.FloatTensor(np.random.normal(0, dim**-0.5, size=(num_embeddings, dim)))\n",
    "    \n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    #Add the oov words to the dict, assigning a new index to each\n",
    "        i = len(glove_embeddings)\n",
    "        for w in oovs:\n",
    "            glove_word2i[w] = i\n",
    "            i +=1\n",
    "    #Concatenate a new row to embeddings for each oov, initialize those new rows with `intialize_new_embedding_weights`\n",
    "        new_emb = initialize_new_embedding_weights(len(oovs), len(glove_embeddings[0]))\n",
    "        cat_emb = torch.cat((glove_embeddings, new_emb), 0)\n",
    "        return (glove_word2i, cat_emb)\n",
    "\n",
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "    embeddings_path,\n",
    "    vocab\n",
    ")\n",
    "oovs = get_oovs(vocab, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9828748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into Train and Dev\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataset(texts, labels, percent_train):\n",
    "    \n",
    "    num_texts = len(texts)\n",
    "    df = pd.DataFrame(\n",
    "    {'text': texts,\n",
    "     'labels': labels\n",
    "    })\n",
    "    train = df.sample(frac=0.8)\n",
    "    for txt in train:\n",
    "        dev = df[df.text != txt]\n",
    "    train_texts = list(train.loc[:,\"text\"])\n",
    "    train_labels = list(train.loc[:,\"labels\"])\n",
    "    dev_texts = list(dev.loc[:,\"text\"])\n",
    "    dev_labels = list(dev.loc[:,\"labels\"])\n",
    "    \n",
    "    return train_texts, train_labels, dev_texts, dev_labels\n",
    "\n",
    "train_sentences, train_labels, dev_texts, dev_labels = split_dataset(args, labs, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60532e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train items: 4176\n",
      "Num dev items: 1044\n"
     ]
    }
   ],
   "source": [
    "print('Num train items:', len(train_arguments))\n",
    "print('Num dev items:', len(dev_arguments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54826dfe",
   "metadata": {},
   "source": [
    "Batch Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e93ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
